{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d3d31e4",
      "metadata": {
        "id": "3d3d31e4"
      },
      "source": [
        "# ASL Neural Network Pipeline Notebook\n",
        "\n",
        "This notebook contains all the steps necessary to train a neural network for the ASL Neural Network App project located at [this repository](https://github.com/TWilliamsA7/asl-neural-app/tree/main). Utility functions can also be found in the above repository under the src directory.\n",
        "\n",
        "1. Setup: Configuration & Authentication\n",
        "2. Environment: Initialization & Imports\n",
        "3. Data: Acquisition & Preprocessing\n",
        "4. Data: Loading & Splitting\n",
        "5. Model: Architecture\n",
        "6. Model: Training\n",
        "7. Model: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc88ffa",
      "metadata": {
        "id": "8fc88ffa"
      },
      "source": [
        "## Setup: Configuration & Authenticatioon\n",
        "\n",
        "This section of the notebook is for setting up the necessary authentication and configuration of the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c745b2",
      "metadata": {
        "id": "a5c745b2"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules for setup\n",
        "\n",
        "from google.colab import userdata, auth, files\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523bde30",
      "metadata": {
        "id": "523bde30"
      },
      "source": [
        "### Create github connection via colab variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7289b94",
      "metadata": {
        "id": "e7289b94"
      },
      "outputs": [],
      "source": [
        "# Define repository details\n",
        "USERNAME = \"TWilliamsA7\"\n",
        "REPO_NAME = \"asl-neural-app.git\"\n",
        "BRANCH_NAME = \"main\"\n",
        "\n",
        "# Get PAT (Personal Access Token) stored in Colab Secrets\n",
        "PAT = userdata.get(\"GITHUB_PAT\")\n",
        "if not PAT:\n",
        "    raise ValueError(\"GITHUB_PAT secret not found!\")\n",
        "\n",
        "# Construct Authetnicated URL for accessing repositry\n",
        "AUTHENTICATED_URL = f\"https://{PAT}@github.com/{USERNAME}/{REPO_NAME}\"\n",
        "REPO_FOLDER = REPO_NAME.replace(\".git\", \"\")\n",
        "\n",
        "# Set global Git configuration\n",
        "!git config --global user.email \"twilliamsa776@gmail.com\"\n",
        "!git config --global user.name \"{USERNAME}\"\n",
        "\n",
        "print(\"Setup github connection and authenticated url successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242d89e6",
      "metadata": {
        "id": "242d89e6"
      },
      "source": [
        "### Google Cloud Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ae645b",
      "metadata": {
        "id": "54ae645b"
      },
      "outputs": [],
      "source": [
        "print(\"--- GCS Authentication ---\")\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Google Cloud authentication complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10250879",
      "metadata": {
        "id": "10250879"
      },
      "source": [
        "## Environment: Initialization and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb5f491",
      "metadata": {
        "id": "ccb5f491"
      },
      "source": [
        "### Clone Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c52c298",
      "metadata": {
        "id": "9c52c298"
      },
      "outputs": [],
      "source": [
        "# Clean up any existing clone\n",
        "if os.path.isdir(REPO_FOLDER):\n",
        "    print(f\"Removing old {REPO_FOLDER} folder...\")\n",
        "    !rm -rf {REPO_FOLDER}\n",
        "\n",
        "# Clone the repository using the authenticated URL\n",
        "print(f\"Cloning repository: {REPO_NAME}...\")\n",
        "!git clone {AUTHENTICATED_URL}\n",
        "\n",
        "# Change directory into the cloned repository\n",
        "%cd {REPO_FOLDER}\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1e3b2f",
      "metadata": {
        "id": "6d1e3b2f"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e364e0",
      "metadata": {
        "id": "41e364e0"
      },
      "outputs": [],
      "source": [
        "print(\"Upgrading pip, setuptools, and wheel...\")\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "print(\"Using preinstalled numpy and tensorflow dependencies\")\n",
        "\n",
        "print(\"Installing remaining project dependencies from requirements.txt...\")\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "!pip install crcmod\n",
        "\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup .Kaggle Directory\n",
        "\n",
        "- Must upload kaggle.json file\n",
        "\n"
      ],
      "metadata": {
        "id": "UxPQq8J-11mw"
      },
      "id": "UxPQq8J-11mw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the credentials file already exists in the expected location\n",
        "if not os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
        "    print(\"Uploading kaggle.json file...\")\n",
        "    # This will open a dialog for you to select and upload your file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Check if the upload was successful\n",
        "    if not uploaded:\n",
        "        print(\"ERROR: kaggle.json was not uploaded.\")\n",
        "    else:\n",
        "        # The uploaded file is now in the current working directory (/content/)\n",
        "        # Proceed to move and secure it.\n",
        "\n",
        "        # 2. Create the required directory\n",
        "        !mkdir -p ~/.kaggle/\n",
        "\n",
        "        # 3. Move the uploaded file into the correct directory\n",
        "        # The key in the uploaded dictionary is the filename (kaggle.json)\n",
        "        # User should upload a file: 'kaggle.json'\n",
        "        !mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "        # 4. Set the correct permissions (CRITICAL)\n",
        "        # Permissions MUST be 600 for security.\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "        print(\"Kaggle authentication file set up successfully!\")\n",
        "else:\n",
        "    print(\"Kaggle credentials already found at ~/.kaggle/kaggle.json.\")\n",
        "\n",
        "# --- Verification Step ---\n",
        "# Run a simple Kaggle command to test authentication\n",
        "try:\n",
        "    print(\"\\nAttempting to list datasets (Verification)...\")\n",
        "    # This command uses the username/key from the now-configured kaggle.json\n",
        "    !kaggle datasets list -s asl_alphabet | head -n 3\n",
        "    print(\"\\nSUCCESS: Kaggle API authenticated and is functional.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Verification failed. Please check the content of your kaggle.json file. Details: {e}\")"
      ],
      "metadata": {
        "id": "KJxd0qhK11Mi"
      },
      "id": "KJxd0qhK11Mi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f773ce51",
      "metadata": {
        "id": "f773ce51"
      },
      "source": [
        "### Connect Src directory for access to utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15926923",
      "metadata": {
        "id": "15926923"
      },
      "outputs": [],
      "source": [
        "sys.path.append('src')\n",
        "print(\"Setup Complete. Colab environment is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c389e19f",
      "metadata": {
        "id": "c389e19f"
      },
      "source": [
        "## Data: Acquisition & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0339e4f8",
      "metadata": {
        "id": "0339e4f8"
      },
      "source": [
        "### Include necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d219f52",
      "metadata": {
        "id": "1d219f52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "# If earlier cells are not ran\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure src accessibility\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import utility functions\n",
        "from data_utils import extract_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112699ad",
      "metadata": {
        "id": "112699ad"
      },
      "source": [
        "### Setup directories and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d527d2c4",
      "metadata": {
        "id": "d527d2c4"
      },
      "outputs": [],
      "source": [
        "KAGGLE_DATASET_ID = \"grassknoted/asl-alphabet\"\n",
        "DESTINATION_PATH = \"sample_data\"\n",
        "PROCESSED_OUTPUT_DIR = 'processed_data'\n",
        "DATA_ROOT_FOLDER_NAME = 'asl_alphabet_train'\n",
        "\n",
        "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
        "os.makedirs(PROCESSED_OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1419d35",
      "metadata": {
        "id": "a1419d35"
      },
      "source": [
        "### Download Data via Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719c8f59",
      "metadata": {
        "id": "719c8f59"
      },
      "outputs": [],
      "source": [
        "print(f\"Downloading dataset: {KAGGLE_DATASET_ID}\")\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_ID} -p {DESTINATION_PATH} --unzip\n",
        "\n",
        "# Define the exact root path to the image subfolders (A, B, C, etc.)\n",
        "DATA_ROOT = os.path.join(DESTINATION_PATH, DATA_ROOT_FOLDER_NAME, DATA_ROOT_FOLDER_NAME)\n",
        "print(f\"Image data root set to: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109baddd",
      "metadata": {
        "id": "109baddd"
      },
      "source": [
        "### Feature Extraction and Array Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c994b52",
      "metadata": {
        "id": "7c994b52"
      },
      "outputs": [],
      "source": [
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "\n",
        "# 1. Get all unique class folder names and sort them alphabetically\n",
        "class_names = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
        "\n",
        "# 2. Create the dictionary\n",
        "label_map = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "FEATURE_OUTPUT_DIR = os.path.join('processed_data', 'class_splits')\n",
        "os.makedirs(FEATURE_OUTPUT_DIR, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "def create_and_save_features():\n",
        "    # List to hold file paths of NPY files for later concatenation\n",
        "    all_class_files = []\n",
        "\n",
        "    # Iterate through all class folders\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(DATA_ROOT, class_name)\n",
        "        label_index = label_map[class_name]\n",
        "\n",
        "        print(f\"Processing Class: {class_name} (Label: {label_index})\")\n",
        "\n",
        "        # --- Memory-Saving Block ---\n",
        "        class_keypoints = []\n",
        "        class_images = []\n",
        "        class_labels = []\n",
        "\n",
        "        for image_name in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "\n",
        "            # Use the imported modular function\n",
        "            keypoints, resized_img = extract_keypoints(image_path)\n",
        "\n",
        "            if keypoints is not None:\n",
        "                class_keypoints.append(keypoints)\n",
        "                class_images.append(resized_img)\n",
        "                class_labels.append(label_index)\n",
        "\n",
        "        # 3. Convert and Save (The memory-intensive part, done one class at a time)\n",
        "        X_key_class = np.array(class_keypoints, dtype=np.float32)\n",
        "        X_cnn_class = np.array(class_images, dtype=np.float32)\n",
        "        y_class = np.array(class_labels, dtype=np.int32)\n",
        "\n",
        "        # 4. Save to Disk\n",
        "        # Use a temporary name for each class file\n",
        "        key_file = os.path.join(FEATURE_OUTPUT_DIR, f'keypoints_{class_name}.npy')\n",
        "        cnn_file = os.path.join(FEATURE_OUTPUT_DIR, f'cnn_{class_name}.npy')\n",
        "        label_file = os.path.join(FEATURE_OUTPUT_DIR, f'labels_{class_name}.npy')\n",
        "\n",
        "        np.save(key_file, X_key_class)\n",
        "        np.save(cnn_file, X_cnn_class)\n",
        "        np.save(label_file, y_class)\n",
        "        all_class_files.append((key_file, cnn_file, label_file))\n",
        "\n",
        "        print(f\"Processed and saved {class_name}. Freeing memory...\")\n",
        "\n",
        "        # 5. Crucial: Delete objects and force garbage collection\n",
        "        del X_key_class, X_cnn_class, y_class, class_keypoints, class_images, class_labels\n",
        "        gc.collect()\n",
        "\n",
        "# --- EXECUTION ---\n",
        "create_and_save_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Concatenation"
      ],
      "metadata": {
        "id": "oIUXwOHPcOCY"
      },
      "id": "oIUXwOHPcOCY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting memory-optimized final concatenation...\")\n",
        "\n",
        "# 1. Identify all temporary class files that need to be merged\n",
        "temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "keypoint_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('keypoints_')]\n",
        "cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "\n",
        "# Check if files were found\n",
        "if not keypoint_files:\n",
        "    raise FileNotFoundError(\"No temporary keypoint files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not cnn_files:\n",
        "    raise FileNotFoundError(\"No temporary cnn files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not label_files:\n",
        "    raise FileNotFoundError(\"No temporary label files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "\n",
        "# 2. Memory-Optimized Concatenation (Loading one-by-one and overwriting)\n",
        "\n",
        "def merge_files_efficiently(file_list, final_name):\n",
        "    \"\"\"Loads files sequentially and saves the final result.\"\"\"\n",
        "\n",
        "    output_path = os.path.join(FEATURE_OUTPUT_DIR, final_name)\n",
        "    print(f\"Merging {len(file_list)} files into {final_name}...\")\n",
        "\n",
        "    all_arrays = [np.load(f) for f in file_list]\n",
        "    merged_array = np.concatenate(all_arrays)\n",
        "    np.save(output_path, merged_array)\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each merge\n",
        "    del all_arrays, merged_array\n",
        "    gc.collect()\n",
        "    print(f\"Successfully saved {final_name}.\")\n",
        "    return output_path\n",
        "\n",
        "# Execute the merges\n",
        "final_keypoints_path = merge_files_efficiently(keypoint_files, 'X_keypoints.npy')\n",
        "final_labels_path = merge_files_efficiently(label_files, 'y_labels.npy')\n",
        "\n",
        "print(\"\\nAll final feature files created successfully on local disk.\")\n",
        "\n",
        "# 3. Upload to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "print(f\"Uploading final processed features from {FEATURE_OUTPUT_DIR} to {GCS_PATH}...\")\n",
        "\n",
        "print(f\"Uploading final feature files to {GCS_PATH}...\")\n",
        "\n",
        "# Upload X_keypoints.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/X_keypoints.npy {GCS_PATH}/X_keypoints.npy\n",
        "\n",
        "# Upload y_labels.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/y_labels.npy {GCS_PATH}/y_labels.npy\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Only final files were uploaded.\")\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Data processing pipeline finished! ðŸŽ‰\")"
      ],
      "metadata": {
        "id": "1rfczEYtcNMr"
      },
      "id": "1rfczEYtcNMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Memory-Mapped Merge for X_cnn_images (with Disk Cleanup) ---\")\n",
        "\n",
        "# 1. Identify all temporary files and verify paths\n",
        "try:\n",
        "    temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "    # We load these lists for reference, they are NOT deleted yet.\n",
        "    label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "    cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The directory {FEATURE_OUTPUT_DIR} was not found. Please check REPO_NAME.\")\n",
        "    exit()\n",
        "\n",
        "if not cnn_files or not label_files:\n",
        "    print(\"Error: No intermediate 'cnn_*.npy' or 'labels_*.npy' files found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# 2. Calculate the required final shape (metadata only)\n",
        "print(f\"Found {len(cnn_files)} intermediate files.\")\n",
        "\n",
        "# Calculate the total number of samples (rows)\n",
        "total_samples = sum(np.load(f).shape[0] for f in label_files)\n",
        "\n",
        "# Get the shape of a single image (e.g., (224, 224, 3))\n",
        "cnn_image_shape = np.load(cnn_files[0]).shape[1:]\n",
        "\n",
        "print(f\"Total Samples to Merge: {total_samples}\")\n",
        "print(f\"Image Feature Shape: {cnn_image_shape}\")\n",
        "\n",
        "# 3. Create and Populate the Memory-Mapped Array\n",
        "FINAL_CNN_PATH = os.path.join(FEATURE_OUTPUT_DIR, 'X_cnn_images.npy')\n",
        "current_row = 0\n",
        "\n",
        "print(f\"Creating memory-mapped file at: {FINAL_CNN_PATH}\")\n",
        "\n",
        "# Create the destination memory-mapped array (mode='w+' means create/write)\n",
        "X_cnn_final_map = np.memmap(\n",
        "    FINAL_CNN_PATH,\n",
        "    dtype=np.float32,\n",
        "    mode='w+',\n",
        "    shape=(total_samples, *cnn_image_shape)\n",
        ")\n",
        "\n",
        "# Iteratively write data into the memory-mapped file\n",
        "for i, cnn_file in enumerate(cnn_files):\n",
        "    # Load one small class array into RAM\n",
        "    X_cnn_class = np.load(cnn_file)\n",
        "    num_samples = X_cnn_class.shape[0]\n",
        "\n",
        "    # Write the small array directly into the correct slice of the large file on disk\n",
        "    X_cnn_final_map[current_row:current_row + num_samples] = X_cnn_class\n",
        "\n",
        "    # Update the row counter\n",
        "    current_row += num_samples\n",
        "\n",
        "    print(f\"  -> Wrote file {i+1}/{len(cnn_files)} ({num_samples} samples).\")\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each loop\n",
        "    del X_cnn_class\n",
        "    gc.collect()\n",
        "\n",
        "    # Flush ensures data is written to disk immediately\n",
        "    X_cnn_final_map.flush()\n",
        "\n",
        "    # --- DISK CLEANUP STEP ---\n",
        "    os.remove(cnn_file)\n",
        "    print(f\"  -> Deleted source file: {os.path.basename(cnn_file)}\")\n",
        "\n",
        "print(\"\\nStep 1 of 2: X_cnn_images successfully merged and saved locally.\")\n",
        "\n",
        "# Final cleanup of the memmap object before GCS upload\n",
        "del X_cnn_final_map\n",
        "gc.collect()\n",
        "\n",
        "# 4. Upload the final file to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "GCS_DESTINATION_FILE = os.path.basename(FINAL_CNN_PATH)\n",
        "\n",
        "print(f\"\\nStep 2 of 2: Uploading {GCS_DESTINATION_FILE} to {GCS_PATH}...\")\n",
        "# Use gsutil cp to copy the local file to the GCS path\n",
        "!gsutil cp {FINAL_CNN_PATH} {GCS_PATH}/{GCS_DESTINATION_FILE}\n",
        "\n",
        "print(\"\\nSUCCESS: X_cnn_images.npy uploaded to GCS.\")"
      ],
      "metadata": {
        "id": "R2Q8P6u-jO_2"
      },
      "id": "R2Q8P6u-jO_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data: Loading and Splitting"
      ],
      "metadata": {
        "id": "x7bKz2iu9tH8"
      },
      "id": "x7bKz2iu9tH8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hE6qsRjJcdbr"
      },
      "id": "hE6qsRjJcdbr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "%cd /content\n",
        "\n",
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1/\"\n",
        "CNN_FILE_NAME = \"X_cnn_images.npy\"\n",
        "KEY_FILE_NAME = \"X_keypoints.npy\"\n",
        "LABELS_FILE_NAME = \"y_labels.npy\"\n",
        "LOCAL_FEATURE_DIR = 'gcs_loaded_data'\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n"
      ],
      "metadata": {
        "id": "HXEDMgUXWVlz"
      },
      "id": "HXEDMgUXWVlz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_gcs_data():\n",
        "    \"\"\"Authenticates GCS access and copies large files to the local Colab SSD.\"\"\"\n",
        "    print(\"Authenticating Google Cloud Storage...\")\n",
        "    try:\n",
        "        # Authenticate the user for GCS access\n",
        "        auth.authenticate_user()\n",
        "    except Exception as e:\n",
        "        print(f\"Authentication failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Create the local directory\n",
        "    os.makedirs(LOCAL_FEATURE_DIR, exist_ok=True)\n",
        "    print(f\"Local storage directory created at: {LOCAL_FEATURE_DIR}\")\n",
        "\n",
        "    # Use gsutil to copy the files to the local SSD\n",
        "    print(f\"Copying {CNN_FILE_NAME} (38 GB) from GCS to local SSD...\")\n",
        "    # It is crucial to use the local SSD for fast I/O during training.\n",
        "    # The 'gsutil cp' command is optimized for this transfer.\n",
        "    try:\n",
        "        # Copy the large feature file\n",
        "        !gsutil cp {GCS_PATH}{CNN_FILE_NAME} {LOCAL_FEATURE_DIR}/\n",
        "\n",
        "        # Copy the much smaller labels file\n",
        "        print(f\"Copying {LABELS_FILE_NAME} from GCS to local SSD...\")\n",
        "        !gsutil cp {GCS_PATH}{LABELS_FILE_NAME} {LOCAL_FEATURE_DIR}/\n",
        "\n",
        "        print(f\"Copying {KEY_FILE_NAME} from GCS to local SSD...\")\n",
        "        !gsutil -m cp {GCS_PATH}{KEY_FILE_NAME} {LOCAL_FEATURE_DIR}/\n",
        "\n",
        "        print(\"Data transfer complete.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Data transfer failed: {e}\")\n",
        "        return False\n",
        "\n",
        "setup_gcs_data()"
      ],
      "metadata": {
        "id": "A5WrkuYxQY4E"
      },
      "id": "A5WrkuYxQY4E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing data loading...\")\n",
        "\n",
        "# Path to files\n",
        "cnn_path = os.path.join(LOCAL_FEATURE_DIR, CNN_FILE_NAME)\n",
        "key_path = os.path.join(LOCAL_FEATURE_DIR, KEY_FILE_NAME)\n",
        "label_path = os.path.join(LOCAL_FEATURE_DIR, LABELS_FILE_NAME)\n",
        "\n",
        "# --- STEP 1: Load Labels to determine Total Samples (N) ---\n",
        "# We try np.load first. If that fails (because it's a raw memmap file),\n",
        "# we calculate N based on file size.\n",
        "\n",
        "try:\n",
        "    # Try standard load (works if you saved with np.save)\n",
        "    y_labels = np.load(label_path)\n",
        "    total_samples = y_labels.shape[0]\n",
        "    print(f\"Loaded Labels via np.load. Total samples: {total_samples}\")\n",
        "except ValueError:\n",
        "    # Fallback: It's a raw binary file from np.memmap\n",
        "    print(\"Labels file is raw binary. Loading via memmap...\")\n",
        "    # Assuming labels are int32 (4 bytes)\n",
        "    file_size = os.path.getsize(label_path)\n",
        "    total_samples = file_size // 4\n",
        "    y_labels = np.memmap(label_path, dtype=np.int32, mode='r', shape=(total_samples,))\n",
        "    print(f\"Loaded Labels via memmap. Total samples: {total_samples}\")\n",
        "\n",
        "# --- STEP 2: Load Keypoints ---\n",
        "try:\n",
        "    X_keypoints = np.load(key_path)\n",
        "except ValueError:\n",
        "    print(\"Keypoints file is raw binary. Loading via memmap...\")\n",
        "    # Shape is (N, 42), float32\n",
        "    X_keypoints = np.memmap(key_path, dtype=np.float32, mode='r', shape=(total_samples, 42))\n",
        "\n",
        "print(f\"X_keypoints shape:  {X_keypoints.shape}\")\n",
        "\n",
        "# --- STEP 3: Load CNN Images ---\n",
        "# We MUST use np.memmap here because we created it with np.memmap\n",
        "# Shape is (N, 224, 224, 3), float32\n",
        "\n",
        "print(\"Mapping large CNN image file (Read-Only)...\")\n",
        "img_shape = (224, 224, 3)\n",
        "\n",
        "X_cnn_mmap = np.memmap(\n",
        "    cnn_path,\n",
        "    dtype=np.float32,\n",
        "    mode='r',\n",
        "    shape=(total_samples, *img_shape)\n",
        ")\n",
        "\n",
        "print(f\"X_cnn_mmap mapped. Shape: {X_cnn_mmap.shape}\")\n",
        "print(\"Data ready for splitting.\")"
      ],
      "metadata": {
        "id": "WrkyHvsQ9wyM"
      },
      "id": "WrkyHvsQ9wyM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "# Load labels\n",
        "# We use mmap if it's the large binary file, or standard load if it's a saved .npy\n",
        "try:\n",
        "    y_labels = np.load(os.path.join(LOCAL_FEATURE_DIR, LABELS_FILE_NAME))\n",
        "except:\n",
        "    # Fallback for raw binary\n",
        "    label_path = os.path.join(LOCAL_FEATURE_DIR, LABELS_FILE_NAME)\n",
        "    file_size = os.path.getsize(label_path)\n",
        "    total_samples = file_size // 4\n",
        "    y_labels = np.memmap(label_path, dtype=np.int32, mode='r', shape=(total_samples,))\n",
        "\n",
        "# Count samples per class\n",
        "counter = collections.Counter(y_labels)\n",
        "print(\"--- Sample Counts Per Class ---\")\n",
        "print(f\"Total Samples: {len(y_labels)}\")\n",
        "print(f\"Total Classes: {len(counter)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Print classes with dangerously low samples (< 10)\n",
        "low_data_classes = []\n",
        "for label, count in sorted(counter.items()):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "    if count < 10:\n",
        "        low_data_classes.append(label)\n",
        "\n",
        "if low_data_classes:\n",
        "    print(f\"\\nCRITICAL WARNING: Classes {low_data_classes} have insufficient data!\")\n",
        "    print(\"Stratified splitting will fail for these classes.\")\n",
        "else:\n",
        "    print(\"\\nData counts look robust. The error might be an edge case.\")"
      ],
      "metadata": {
        "id": "SGeo8GlKnch0"
      },
      "id": "SGeo8GlKnch0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Generate a list of indices [0, 1, 2, ... N-1]\n",
        "num_samples = y_labels.shape[0]\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "print(\"Attempting Robust Data Split...\")\n",
        "\n",
        "try:\n",
        "    # 1. Split Indices into Train and Temp (Validation + Test)\n",
        "    # We try stratification first\n",
        "    train_idx, temp_idx, y_train_sparse, y_temp_sparse = train_test_split(\n",
        "        indices, y_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "    print(\"Primary split (Train/Temp) successful with stratification.\")\n",
        "\n",
        "    # 2. Split Temp Indices into Validation and Test\n",
        "    try:\n",
        "        val_idx, test_idx, y_val_sparse, y_test_sparse = train_test_split(\n",
        "            temp_idx, y_temp_sparse,\n",
        "            test_size=0.5, # 50% of 20% = 10% total\n",
        "            random_state=42,\n",
        "            stratify=y_temp_sparse\n",
        "        )\n",
        "        print(\"Secondary split (Val/Test) successful with stratification.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"WARNING: Stratified split failed for Val/Test ({e}).\")\n",
        "        print(\"Falling back to random split (non-stratified) for Validation/Test sets.\")\n",
        "        val_idx, test_idx, y_val_sparse, y_test_sparse = train_test_split(\n",
        "            temp_idx, y_temp_sparse,\n",
        "            test_size=0.5,\n",
        "            random_state=42,\n",
        "            stratify=None # Disable stratification to prevent crash\n",
        "        )\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"CRITICAL ERROR: Even primary split failed. Data is extremely sparse. {e}\")\n",
        "    # Fallback to completely random split if data is essentially empty for some classes\n",
        "    train_idx, temp_idx, y_train_sparse, y_temp_sparse = train_test_split(\n",
        "        indices, y_labels, test_size=0.2, random_state=42, stratify=None\n",
        "    )\n",
        "    val_idx, test_idx, y_val_sparse, y_test_sparse = train_test_split(\n",
        "        temp_idx, y_temp_sparse, test_size=0.5, random_state=42, stratify=None\n",
        "    )\n",
        "\n",
        "# 3. One-Hot Encode Labels\n",
        "num_classes = len(np.unique(y_labels))\n",
        "y_labels_categorical = to_categorical(y_labels, num_classes=num_classes)\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(f\"Training Indices:   {len(train_idx)}\")\n",
        "print(f\"Validation Indices: {len(val_idx)}\")\n",
        "print(f\"Test Indices:       {len(test_idx)}\")"
      ],
      "metadata": {
        "id": "xtuIQNeJlJwd"
      },
      "id": "xtuIQNeJlJwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class MultiInputGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    The canonical Keras Sequence for loading batches of multi-input data.\n",
        "    This structure is mandatory for stability when the tf.data wrapper causes issues.\n",
        "    \"\"\"\n",
        "    def __init__(self, indices, x_cnn_mmap, x_keypoints, y_labels_cat, batch_size=64, shuffle=True):\n",
        "        self.indices = indices\n",
        "        self.x_cnn_mmap = x_cnn_mmap\n",
        "        self.x_keypoints = x_keypoints\n",
        "        self.y_labels_cat = y_labels_cat\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        # on_epoch_end is called immediately by Keras at the start of training\n",
        "        self.on_epoch_end()\n",
        "        if len(self.indices) == 0:\n",
        "            raise ValueError(\"Indices list is empty. Check your data splitting in Cell 4.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # This method is what Keras uses to determine the number of steps\n",
        "        # (if steps_per_epoch is not explicitly provided)\n",
        "        return math.ceil(len(self.indices) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = (index + 1) * self.batch_size\n",
        "        batch_indices = self.indices[start_index:end_index]\n",
        "\n",
        "        # Gather data (NumPy arrays/memory maps are accessed here)\n",
        "        batch_images = self.x_cnn_mmap[batch_indices]\n",
        "        batch_keypoints = self.x_keypoints[batch_indices]\n",
        "        batch_labels = self.y_labels_cat[batch_indices]\n",
        "\n",
        "        # Explicitly convert to tf.Tensor\n",
        "        tensor_images = tf.convert_to_tensor(batch_images)\n",
        "        tensor_keypoints = tf.convert_to_tensor(batch_keypoints)\n",
        "        tensor_labels = tf.convert_to_tensor(batch_labels)\n",
        "\n",
        "        # Return inputs as a dictionary, mapping to layer names\n",
        "        inputs_dict = {\n",
        "            \"image_input\": tensor_images,\n",
        "            \"keypoint_input\": tensor_keypoints\n",
        "        }\n",
        "\n",
        "        # Return: (inputs_dictionary, labels_tensor)\n",
        "        return inputs_dict, tensor_labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Called by Keras after every epoch. This ensures the indices are shuffled\n",
        "        before the next epoch starts, which is mandatory for training stability.\n",
        "        \"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "# --- 1. Create Keras Sequence Objects ---\n",
        "BATCH_SIZE = 64\n",
        "num_classes = y_labels_categorical.shape[1]\n",
        "\n",
        "# Note: These are now the simple Sequence objects.\n",
        "train_gen_seq = MultiInputGenerator(train_idx, X_cnn_mmap, X_keypoints, y_labels_categorical, BATCH_SIZE, shuffle=True)\n",
        "val_gen_seq = MultiInputGenerator(val_idx, X_cnn_mmap, X_keypoints, y_labels_categorical, BATCH_SIZE, shuffle=False)\n",
        "test_gen_seq = MultiInputGenerator(test_idx, X_cnn_mmap, X_keypoints, y_labels_categorical, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- 2. Create Optimized tf.data.Dataset ONLY FOR VALIDATION/TEST (Optional) ---\n",
        "# We keep this for validation because it works and offers minor efficiency gains.\n",
        "\n",
        "# Define the expected output signature\n",
        "output_signature = (\n",
        "    {\n",
        "        \"image_input\": tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        \"keypoint_input\": tf.TensorSpec(shape=(None, 42), dtype=tf.float32)\n",
        "    },\n",
        "    tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "def sequence_to_dataset(sequence, signature):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: sequence,\n",
        "        output_signature=signature\n",
        "    )\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: (x, y),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# The training set is NOT converted to tf.data.Dataset\n",
        "val_ds = sequence_to_dataset(val_gen_seq, output_signature)\n",
        "test_ds = sequence_to_dataset(test_gen_seq, output_signature)\n",
        "\n",
        "print(\"Training Data: Keras Sequence (train_gen_seq)\")\n",
        "print(\"Validation Data: Optimized tf.data.Dataset (val_ds)\")\n",
        "print(\"This configuration forces reliable epoch-end state management.\")"
      ],
      "metadata": {
        "id": "sSSf5qLRlr1G"
      },
      "id": "sSSf5qLRlr1G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_fusion_model(num_classes, learning_rate=1e-4):\n",
        "    # --- Branch 1: CNN for Images (MobileNetV2) ---\n",
        "    # Input shape: 224x224x3 RGB\n",
        "    input_image = Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "\n",
        "    # Load MobileNetV2, exclude top classification layer\n",
        "    base_mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_image)\n",
        "\n",
        "    # Freeze the base model initially (optional, but recommended for stability)\n",
        "    base_mobilenet.trainable = False\n",
        "\n",
        "    x1 = base_mobilenet.output\n",
        "    x1 = GlobalAveragePooling2D()(x1)\n",
        "    x1 = Dense(128, activation='relu')(x1)\n",
        "    x1 = Dropout(0.3)(x1)\n",
        "\n",
        "    # --- Branch 2: MLP for Keypoints ---\n",
        "    # Input shape: 42 (21 points * 2 coordinates)\n",
        "    input_keypoints = Input(shape=(42,), name=\"keypoint_input\")\n",
        "\n",
        "    x2 = Dense(64, activation='relu')(input_keypoints)\n",
        "    x2 = Dropout(0.3)(x2)\n",
        "    x2 = Dense(32, activation='relu')(x2)\n",
        "\n",
        "    # --- Fusion ---\n",
        "    combined = Concatenate()([x1, x2])\n",
        "\n",
        "    # Final Classification Head\n",
        "    z = Dense(64, activation='relu')(combined)\n",
        "    z = Dropout(0.2)(z)\n",
        "    output = Dense(num_classes, activation='softmax', name=\"class_output\")(z)\n",
        "\n",
        "    # Create Model\n",
        "    model = Model(inputs=[input_image, input_keypoints], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize Model\n",
        "model = create_fusion_model(num_classes=num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nOCE3hasltjd"
      },
      "id": "nOCE3hasltjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load previous weights"
      ],
      "metadata": {
        "id": "c55C6lcvOUKf"
      },
      "id": "c55C6lcvOUKf"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load the Best Weights ---\n",
        "# Ensure your model architecture is defined (Cell 6) and compiled (Cell 7) BEFORE running this.\n",
        "try:\n",
        "    # Use the name of the file saved by your ModelCheckpoint/BackupAndRestore\n",
        "    model.load_weights('processed_features_v1_latest.weights.h5')\n",
        "    print(\"Successfully loaded model weights.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load weights file. Please check the path: {e}\")\n",
        "    # Halt execution if weights cannot be loaded, as the rest of the process is meaningless.\n",
        "    raise\n",
        "\n",
        "# --- 2. Fix the Optimizer State Mismatch ---\n",
        "\n",
        "print(\"Initializing Adam optimizer state to match loaded weights...\")\n",
        "\n",
        "# CRITICAL STEP: Run a dummy training step.\n",
        "# This forces the Keras Adam optimizer to allocate its 20 internal state variables\n",
        "# (momentum and velocity vectors for every weight) after the weights have been loaded.\n",
        "\n",
        "@tf.function\n",
        "def initialize_optimizer_state():\n",
        "    \"\"\"Runs a single, no-op gradient calculation to allocate Adam's state variables.\"\"\"\n",
        "    # We grab one batch's shape from the generator objects defined in Cell 5.\n",
        "    # Note: We use the *shape* and *dtype* defined in the generator output, not actual data.\n",
        "\n",
        "    # Use the shapes you defined for the inputs in the generator:\n",
        "    image_shape = (1, 224, 224, 3)\n",
        "    keypoint_shape = (1, 42)\n",
        "    label_shape = (1, num_classes) # num_classes should be defined from y_labels_categorical.shape[1]\n",
        "\n",
        "    # Create a batch of Zeros with size 1\n",
        "    dummy_input = {\n",
        "        \"image_input\": tf.zeros(image_shape, dtype=tf.float32),\n",
        "        \"keypoint_input\": tf.zeros(keypoint_shape, dtype=tf.float32)\n",
        "    }\n",
        "    dummy_label = tf.zeros(label_shape, dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        predictions = model(dummy_input, training=True)\n",
        "        # Calculate loss (no need for a real value, just need to trigger the computation graph)\n",
        "        # Using the recommended method `model.compute_loss` to replace the deprecated `model.compiled_loss`\n",
        "        loss = model.compute_loss(y=dummy_label, y_pred=predictions)\n",
        "\n",
        "    # Compute and apply gradients (this is the step that allocates the M/V variables)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "# Execute the initialization\n",
        "initialize_optimizer_state()\n",
        "print(\"Optimizer state successfully initialized. The 2 vs. 22 variable mismatch is resolved.\")\n",
        "\n",
        "# --- 3. Resume Training ---\n",
        "# You can now proceed to the model.fit() call in your next block (Cell 7).\n",
        "# This time, Adam will start training with momentum from scratch, but it will not\n",
        "# throw the variable mismatch warning and should stabilize quickly."
      ],
      "metadata": {
        "id": "5tUAESGWOW-y"
      },
      "id": "5tUAESGWOW-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, BackupAndRestore\n",
        "\n",
        "# Checkpoint to save model\n",
        "backup_restore = BackupAndRestore(\n",
        "    backup_dir=GCS_PATH, # MUST be a GCS path or local folder\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "\n",
        "# Define Callbacks\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_fusion_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 64 # Confirm this matches Cell 5\n",
        "TRAIN_SAMPLES = len(train_idx) # train_idx comes from Cell 4\n",
        "VAL_SAMPLES = len(val_idx) # Get validation samples\n",
        "\n",
        "# The number of steps is the ceiling of the total samples divided by the batch size.\n",
        "STEPS_PER_EPOCH = math.ceil(TRAIN_SAMPLES / BATCH_SIZE)\n",
        "VALIDATION_STEPS = math.ceil(VAL_SAMPLES / BATCH_SIZE)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_gen_seq,\n",
        "    validation_data=val_ds,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    epochs=25, # Adjust as needed\n",
        "    callbacks=[backup_restore, checkpoint, early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "UWSYG5pslycm"
      },
      "id": "UWSYG5pslycm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd # Used for better visualization of the confusion matrix\n",
        "\n",
        "# --- 1. Define Constants (Must match Cell 5/7) ---\n",
        "BATCH_SIZE = 64\n",
        "TEST_SAMPLES = len(test_idx) # Assumes test_idx is available from Cell 4\n",
        "\n",
        "TEST_STEPS = math.ceil(TEST_SAMPLES / BATCH_SIZE)\n",
        "\n",
        "# --- 2. Load the Best Model ---\n",
        "MODEL_PATH = 'best_fusion_model.keras'\n",
        "\n",
        "try:\n",
        "    # Load the best model saved by the ModelCheckpoint callback in Cell 7.\n",
        "    # This ensures we are testing the highest-performing version.\n",
        "    final_model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    print(f\"Successfully loaded the best model for evaluation from: {MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model from {MODEL_PATH}: {e}\")\n",
        "    print(\"WARNING: Using the last state of the currently loaded model instead.\")\n",
        "    # Assuming 'model' variable is still holding the trained model object from Cell 7\n",
        "    final_model = model\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Evaluation Details: {TEST_SAMPLES} samples in {TEST_STEPS} steps.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 3. Evaluate the Model (Standard Metrics) ---\n",
        "# Use the optimized tf.data.Dataset (test_ds) for evaluation.\n",
        "print(\"Starting final model evaluation on the unseen test dataset...\")\n",
        "\n",
        "results = final_model.evaluate(\n",
        "    test_ds,\n",
        "    steps=TEST_STEPS,\n",
        "    verbose=1,\n",
        "    return_dict=True # Return results as a dictionary for clear output\n",
        ")\n",
        "\n",
        "# --- 4. Display Standard Results ---\n",
        "print(\"\\n--- Final Test Evaluation Results ---\")\n",
        "for name, value in results.items():\n",
        "    if name == 'accuracy':\n",
        "        print(f\"TEST ACCURACY: {value:.4f} (This is the final performance metric)\")\n",
        "    else:\n",
        "        print(f\"Test {name.capitalize()}: {value:.4f}\")\n",
        "print(\"-------------------------------------\")\n",
        "\n",
        "\n",
        "# --- 5. DETAILED CLASSIFICATION ANALYSIS ---\n",
        "\n",
        "# 5a. Get Raw Predictions\n",
        "print(\"\\n--- Generating Raw Predictions ---\")\n",
        "test_predictions = final_model.predict(\n",
        "    test_ds,\n",
        "    steps=TEST_STEPS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5b. Get True Labels\n",
        "# Extract the true labels from the original one-hot array using the test indices.\n",
        "y_test_one_hot = y_labels_categorical[test_idx]\n",
        "\n",
        "# 5c. Convert to Class Indices (Integers)\n",
        "# Convert one-hot encoded true labels to single class indices (0, 1, 2, ...)\n",
        "y_true_classes = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "# Convert predicted probabilities/logits to single class indices (0, 1, 2, ...)\n",
        "y_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Ensure the lengths match\n",
        "if len(y_true_classes) != len(y_pred_classes):\n",
        "    # This happens if the generator or dataset padded the last batch.\n",
        "    # We must truncate the predictions to match the true label count.\n",
        "    min_len = min(len(y_true_classes), len(y_pred_classes))\n",
        "    y_pred_classes = y_pred_classes[:min_len]\n",
        "    y_true_classes = y_true_classes[:min_len]\n",
        "    print(f\"Warning: Prediction length truncated to {min_len} to match true label count.\")\n",
        "\n",
        "# 5d. Generate and Print Classification Report\n",
        "print(\"\\n\\n--- CLASSIFICATION REPORT ---\")\n",
        "# The target_names should ideally be the list of your ASL signs (e.g., ['A', 'B', 'C', ...])\n",
        "# Replace `[str(i) for i in range(y_test_one_hot.shape[1])]` with your actual class names if known.\n",
        "class_names = [str(i) for i in range(y_test_one_hot.shape[1])]\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names, zero_division=0))\n",
        "\n",
        "# 5e. Generate and Print Confusion Matrix\n",
        "print(\"\\n\\n--- CONFUSION MATRIX ---\")\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "print(\"Rows = True Class, Columns = Predicted Class\")\n",
        "\n",
        "# Display the confusion matrix using pandas for better formatting\n",
        "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "print(cm_df)\n",
        "\n",
        "print(\"\\nDetailed analysis complete.\")"
      ],
      "metadata": {
        "id": "341k6YNopxNb"
      },
      "id": "341k6YNopxNb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}