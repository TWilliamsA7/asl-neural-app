{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d3d31e4",
      "metadata": {
        "id": "3d3d31e4"
      },
      "source": [
        "# ASL Neural Network Pipeline Notebook\n",
        "\n",
        "This notebook contains all the steps necessary to train a neural network for the ASL Neural Network App project located at [this repository](https://github.com/TWilliamsA7/asl-neural-app/tree/main). Utility functions can also be found in the above repository under the src directory.\n",
        "\n",
        "1. Setup: Configuration & Authentication\n",
        "2. Environment: Initialization & Imports\n",
        "3. Data: Acquisition & Preprocessing\n",
        "4. Data: Loading & Splitting\n",
        "5. Model: Architecture\n",
        "6. Model: Training\n",
        "7. Model: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc88ffa",
      "metadata": {
        "id": "8fc88ffa"
      },
      "source": [
        "## Setup: Configuration & Authenticatioon\n",
        "\n",
        "This section of the notebook is for setting up the necessary authentication and configuration of the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a5c745b2",
      "metadata": {
        "id": "a5c745b2"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules for setup\n",
        "\n",
        "from google.colab import userdata, auth\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523bde30",
      "metadata": {
        "id": "523bde30"
      },
      "source": [
        "### Create github connection via colab variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e7289b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7289b94",
        "outputId": "41383181-9f83-4510-8a29-0a16f7604180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup github connection and authenticated url successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define repository details\n",
        "USERNAME = \"TWilliamsA7\"\n",
        "REPO_NAME = \"asl-neural-app.git\"\n",
        "BRANCH_NAME = \"main\"\n",
        "\n",
        "# Get PAT (Personal Access Token) stored in Colab Secrets\n",
        "PAT = userdata.get(\"GITHUB_PAT\")\n",
        "if not PAT:\n",
        "    raise ValueError(\"GITHUB_PAT secret not found!\")\n",
        "\n",
        "# Construct Authetnicated URL for accessing repositry\n",
        "AUTHENTICATED_URL = f\"https://{PAT}@github.com/{USERNAME}/{REPO_NAME}\"\n",
        "REPO_FOLDER = REPO_NAME.replace(\".git\", \"\")\n",
        "\n",
        "# Set golabl Git configuration\n",
        "!git config --global user.email \"twilliamsa776@gmail.com\"\n",
        "!git config --global user.name \"{USERNAME}\"\n",
        "\n",
        "print(\"Setup github connection and authenticated url successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242d89e6",
      "metadata": {
        "id": "242d89e6"
      },
      "source": [
        "### Google Cloud Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54ae645b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ae645b",
        "outputId": "032d4622-fce8-4fe4-e474-64eb03a61fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GCS Authentication ---\n",
            "Google Cloud authentication complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- GCS Authentication ---\")\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Google Cloud authentication complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10250879",
      "metadata": {
        "id": "10250879"
      },
      "source": [
        "## Environment: Initialization and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb5f491",
      "metadata": {
        "id": "ccb5f491"
      },
      "source": [
        "### Clone Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c52c298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c52c298",
        "outputId": "21e90d42-c143-4c75-a9ea-f47811ce6f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing old asl-neural-app folder...\n",
            "Cloning repository: asl-neural-app.git...\n",
            "Cloning into 'asl-neural-app'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 92 (delta 39), reused 80 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (92/92), 19.53 KiB | 9.76 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/asl-neural-app\n",
            "Current working directory: /content/asl-neural-app\n"
          ]
        }
      ],
      "source": [
        "# Clean up any existing clone (optional, but good for reliable restarts)\n",
        "if os.path.isdir(REPO_FOLDER):\n",
        "    print(f\"Removing old {REPO_FOLDER} folder...\")\n",
        "    !rm -rf {REPO_FOLDER}\n",
        "\n",
        "# Clone the repository using the authenticated URL\n",
        "print(f\"Cloning repository: {REPO_NAME}...\")\n",
        "!git clone {AUTHENTICATED_URL}\n",
        "\n",
        "# Change directory into the cloned repository\n",
        "%cd {REPO_FOLDER}\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1e3b2f",
      "metadata": {
        "id": "6d1e3b2f"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        "- Includes manual inclusion of kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "41e364e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e364e0",
        "outputId": "6fd29ddc-f41d-4db5-b876-462bdb14082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upgrading pip, setuptools, and wheel...\n",
            "Using preinstalled numpy and tensorflow dependencies\n",
            "Installing remaining project dependencies from requirements.txt...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed successfully.\n",
            "\n",
            "--- MANUAL STEP REQUIRED ---\n",
            "1. Click the Folder icon (left sidebar).\n",
            "2. Navigate to the root folder (click the / symbol).\n",
            "3. Navigate to the hidden folder: .kaggle\n",
            "4. Upload your 'kaggle.json' file into the .kaggle folder.\n",
            "Proceed only after kaggle.json is uploaded.\n"
          ]
        }
      ],
      "source": [
        "print(\"Upgrading pip, setuptools, and wheel...\")\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "print(\"Using preinstalled numpy and tensorflow dependencies\")\n",
        "\n",
        "print(\"Installing remaining project dependencies from requirements.txt...\")\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "print(\"Dependencies installed successfully.\")\n",
        "\n",
        "# Install Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# --- IMPORTANT: Manually upload kaggle.json to the ~/.kaggle folder now ---\n",
        "print(\"\\n--- MANUAL STEP REQUIRED ---\")\n",
        "print(\"1. Click the Folder icon (left sidebar).\")\n",
        "print(\"2. Navigate to the root folder (click the / symbol).\")\n",
        "print(\"3. Navigate to the hidden folder: .kaggle\")\n",
        "print(\"4. Upload your 'kaggle.json' file into the .kaggle folder.\")\n",
        "print(\"Proceed only after kaggle.json is uploaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f773ce51",
      "metadata": {
        "id": "f773ce51"
      },
      "source": [
        "### Connect Src directory for access to utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15926923",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15926923",
        "outputId": "dfc645bb-3bc2-4393-83be-6fe6f14b000b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup Complete. Colab environment is ready.\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('src')\n",
        "print(\"Setup Complete. Colab environment is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c389e19f",
      "metadata": {
        "id": "c389e19f"
      },
      "source": [
        "## Data: Acquisition & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0339e4f8",
      "metadata": {
        "id": "0339e4f8"
      },
      "source": [
        "### Include necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d219f52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d219f52",
        "outputId": "c07704ac-2c00-4a70-c486-a9f30593d6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# If earlier cells are not ran\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure src accessibility\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import utility functions\n",
        "from data_utils import extract_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112699ad",
      "metadata": {
        "id": "112699ad"
      },
      "source": [
        "### Setup directories and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d527d2c4",
      "metadata": {
        "id": "d527d2c4"
      },
      "outputs": [],
      "source": [
        "KAGGLE_DATASET_ID = \"grassknoted/asl-alphabet\"\n",
        "DESTINATION_PATH = \"sample_data\"\n",
        "PROCESSED_OUTPUT_DIR = 'processed_data'\n",
        "DATA_ROOT_FOLDER_NAME = 'asl_alphabet_train'\n",
        "\n",
        "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
        "os.makedirs(PROCESSED_OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1419d35",
      "metadata": {
        "id": "a1419d35"
      },
      "source": [
        "### Download Data via Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "719c8f59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719c8f59",
        "outputId": "b37e34ee-272f-49e1-ccfe-6680e26193b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset: grassknoted/asl-alphabet\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "Image data root set to: sample_data/asl_alphabet_train\n"
          ]
        }
      ],
      "source": [
        "print(f\"Downloading dataset: {KAGGLE_DATASET_ID}\")\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_ID} -p {DESTINATION_PATH} --unzip\n",
        "\n",
        "# Define the exact root path to the image subfolders (A, B, C, etc.)\n",
        "DATA_ROOT = os.path.join(DESTINATION_PATH, DATA_ROOT_FOLDER_NAME)\n",
        "print(f\"Image data root set to: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109baddd",
      "metadata": {
        "id": "109baddd"
      },
      "source": [
        "### Feature Extraction and Array Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c994b52",
      "metadata": {
        "id": "7c994b52"
      },
      "outputs": [],
      "source": [
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "\n",
        "def create_and_save_features():\n",
        "    X_keypoints, X_cnn, y_labels = [], [], []\n",
        "\n",
        "    # Iterate through all class folders\n",
        "    for label_index, class_name in enumerate(sorted(os.listdir(DATA_ROOT))):\n",
        "        class_path = os.path.join(DATA_ROOT, class_name)\n",
        "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing Class: {class_name} (Label: {label_index})\")\n",
        "\n",
        "        for image_name in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "\n",
        "            # Use the imported modular function\n",
        "            keypoints, resized_img = extract_keypoints(image_path)\n",
        "\n",
        "            if keypoints is not None:\n",
        "                # Store keypoints\n",
        "                X_keypoints.append(keypoints)\n",
        "                # Normalize and store CNN image data\n",
        "                X_cnn.append(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB) / 255.0)\n",
        "                y_labels.append(label_index)\n",
        "\n",
        "    # Convert to final NumPy arrays\n",
        "    X_keypoints_array = np.array(X_keypoints, dtype=np.float32)\n",
        "    X_cnn_array = np.array(X_cnn, dtype=np.float32)\n",
        "    y_labels_array = np.array(y_labels, dtype=np.int32)\n",
        "\n",
        "    TEMP_DIR = 'temp_feature_dump'\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "    # Save the processed data to the designated output folder\n",
        "    np.save(os.path.join(TEMP_DIR, 'X_keypoints.npy'), X_keypoints_array)\n",
        "    np.save(os.path.join(TEMP_DIR, 'X_cnn_images.npy'), X_cnn_array)\n",
        "    np.save(os.path.join(TEMP_DIR, 'y_labels.npy'), y_labels_array)\n",
        "\n",
        "    # Source is the local temp directory. Destination is the GCS path.\n",
        "    GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "    print(f\"\\nUploading processed features to {GCS_PATH}...\")\n",
        "\n",
        "    # The -m flag runs the command multi-threaded (faster) and -r copies the directory recursively\n",
        "    !gsutil -m cp -r {TEMP_DIR} {GCS_PATH}\n",
        "\n",
        "    print(\"\\nUpload to GCS complete. Features are ready for training notebook.\")\n",
        "\n",
        "    print(f\"\\nSuccessfully processed {len(X_keypoints)} samples.\")\n",
        "    print(f\"Keypoints Shape: {X_keypoints_array.shape}, Images Shape: {X_cnn_array.shape}\")\n",
        "    print(\"All processed arrays saved to the 'processed_data' directory.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "create_and_save_features()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}