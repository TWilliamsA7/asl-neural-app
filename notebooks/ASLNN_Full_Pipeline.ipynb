{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d3d31e4",
      "metadata": {
        "id": "3d3d31e4"
      },
      "source": [
        "# ASL Neural Network Pipeline Notebook\n",
        "\n",
        "This notebook contains all the steps necessary to train a neural network for the ASL Neural Network App project located at [this repository](https://github.com/TWilliamsA7/asl-neural-app/tree/main). Utility functions can also be found in the above repository under the src directory.\n",
        "\n",
        "1. Setup: Configuration & Authentication\n",
        "2. Environment: Initialization & Imports\n",
        "3. Data: Acquisition & Preprocessing\n",
        "4. Data: Loading & Splitting\n",
        "5. Model: Architecture\n",
        "6. Model: Training\n",
        "7. Model: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc88ffa",
      "metadata": {
        "id": "8fc88ffa"
      },
      "source": [
        "## Setup: Configuration & Authenticatioon\n",
        "\n",
        "This section of the notebook is for setting up the necessary authentication and configuration of the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a5c745b2",
      "metadata": {
        "id": "a5c745b2"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules for setup\n",
        "\n",
        "from google.colab import userdata, auth, files\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523bde30",
      "metadata": {
        "id": "523bde30"
      },
      "source": [
        "### Create github connection via colab variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7289b94",
      "metadata": {
        "id": "e7289b94"
      },
      "outputs": [],
      "source": [
        "# Define repository details\n",
        "USERNAME = \"TWilliamsA7\"\n",
        "REPO_NAME = \"asl-neural-app.git\"\n",
        "BRANCH_NAME = \"main\"\n",
        "\n",
        "# Get PAT (Personal Access Token) stored in Colab Secrets\n",
        "PAT = userdata.get(\"GITHUB_PAT\")\n",
        "if not PAT:\n",
        "    raise ValueError(\"GITHUB_PAT secret not found!\")\n",
        "\n",
        "# Construct Authetnicated URL for accessing repositry\n",
        "AUTHENTICATED_URL = f\"https://{PAT}@github.com/{USERNAME}/{REPO_NAME}\"\n",
        "REPO_FOLDER = REPO_NAME.replace(\".git\", \"\")\n",
        "\n",
        "# Set global Git configuration\n",
        "!git config --global user.email \"twilliamsa776@gmail.com\"\n",
        "!git config --global user.name \"{USERNAME}\"\n",
        "\n",
        "print(\"Setup github connection and authenticated url successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242d89e6",
      "metadata": {
        "id": "242d89e6"
      },
      "source": [
        "### Google Cloud Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ae645b",
      "metadata": {
        "id": "54ae645b"
      },
      "outputs": [],
      "source": [
        "print(\"--- GCS Authentication ---\")\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Google Cloud authentication complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10250879",
      "metadata": {
        "id": "10250879"
      },
      "source": [
        "## Environment: Initialization and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb5f491",
      "metadata": {
        "id": "ccb5f491"
      },
      "source": [
        "### Clone Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c52c298",
      "metadata": {
        "id": "9c52c298"
      },
      "outputs": [],
      "source": [
        "# Clean up any existing clone\n",
        "if os.path.isdir(REPO_FOLDER):\n",
        "    print(f\"Removing old {REPO_FOLDER} folder...\")\n",
        "    !rm -rf {REPO_FOLDER}\n",
        "\n",
        "# Clone the repository using the authenticated URL\n",
        "print(f\"Cloning repository: {REPO_NAME}...\")\n",
        "!git clone {AUTHENTICATED_URL}\n",
        "\n",
        "# Change directory into the cloned repository\n",
        "%cd {REPO_FOLDER}\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1e3b2f",
      "metadata": {
        "id": "6d1e3b2f"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e364e0",
      "metadata": {
        "id": "41e364e0"
      },
      "outputs": [],
      "source": [
        "print(\"Upgrading pip, setuptools, and wheel...\")\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "print(\"Using preinstalled numpy and tensorflow dependencies\")\n",
        "\n",
        "print(\"Installing remaining project dependencies from requirements.txt...\")\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup .Kaggle Directory\n",
        "\n",
        "- Must upload kaggle.json file\n",
        "\n"
      ],
      "metadata": {
        "id": "UxPQq8J-11mw"
      },
      "id": "UxPQq8J-11mw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the credentials file already exists in the expected location\n",
        "if not os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
        "    print(\"Uploading kaggle.json file...\")\n",
        "    # This will open a dialog for you to select and upload your file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Check if the upload was successful\n",
        "    if not uploaded:\n",
        "        print(\"ERROR: kaggle.json was not uploaded.\")\n",
        "    else:\n",
        "        # The uploaded file is now in the current working directory (/content/)\n",
        "        # Proceed to move and secure it.\n",
        "\n",
        "        # 2. Create the required directory\n",
        "        !mkdir -p ~/.kaggle/\n",
        "\n",
        "        # 3. Move the uploaded file into the correct directory\n",
        "        # The key in the uploaded dictionary is the filename (kaggle.json)\n",
        "        # User should upload a file: 'kaggle.json'\n",
        "        !mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "        # 4. Set the correct permissions (CRITICAL)\n",
        "        # Permissions MUST be 600 for security.\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "        print(\"Kaggle authentication file set up successfully!\")\n",
        "else:\n",
        "    print(\"Kaggle credentials already found at ~/.kaggle/kaggle.json.\")\n",
        "\n",
        "# --- Verification Step ---\n",
        "# Run a simple Kaggle command to test authentication\n",
        "try:\n",
        "    print(\"\\nAttempting to list datasets (Verification)...\")\n",
        "    # This command uses the username/key from the now-configured kaggle.json\n",
        "    !kaggle datasets list -s asl_alphabet | head -n 3\n",
        "    print(\"\\nSUCCESS: Kaggle API authenticated and is functional.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Verification failed. Please check the content of your kaggle.json file. Details: {e}\")"
      ],
      "metadata": {
        "id": "KJxd0qhK11Mi"
      },
      "id": "KJxd0qhK11Mi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f773ce51",
      "metadata": {
        "id": "f773ce51"
      },
      "source": [
        "### Connect Src directory for access to utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15926923",
      "metadata": {
        "id": "15926923"
      },
      "outputs": [],
      "source": [
        "sys.path.append('src')\n",
        "print(\"Setup Complete. Colab environment is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c389e19f",
      "metadata": {
        "id": "c389e19f"
      },
      "source": [
        "## Data: Acquisition & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0339e4f8",
      "metadata": {
        "id": "0339e4f8"
      },
      "source": [
        "### Include necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d219f52",
      "metadata": {
        "id": "1d219f52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "# If earlier cells are not ran\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure src accessibility\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import utility functions\n",
        "from data_utils import extract_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112699ad",
      "metadata": {
        "id": "112699ad"
      },
      "source": [
        "### Setup directories and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d527d2c4",
      "metadata": {
        "id": "d527d2c4"
      },
      "outputs": [],
      "source": [
        "KAGGLE_DATASET_ID = \"grassknoted/asl-alphabet\"\n",
        "DESTINATION_PATH = \"sample_data\"\n",
        "PROCESSED_OUTPUT_DIR = 'processed_data'\n",
        "DATA_ROOT_FOLDER_NAME = 'asl_alphabet_train'\n",
        "\n",
        "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
        "os.makedirs(PROCESSED_OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1419d35",
      "metadata": {
        "id": "a1419d35"
      },
      "source": [
        "### Download Data via Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719c8f59",
      "metadata": {
        "id": "719c8f59"
      },
      "outputs": [],
      "source": [
        "print(f\"Downloading dataset: {KAGGLE_DATASET_ID}\")\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_ID} -p {DESTINATION_PATH} --unzip\n",
        "\n",
        "# Define the exact root path to the image subfolders (A, B, C, etc.)\n",
        "DATA_ROOT = os.path.join(DESTINATION_PATH, DATA_ROOT_FOLDER_NAME, DATA_ROOT_FOLDER_NAME)\n",
        "print(f\"Image data root set to: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109baddd",
      "metadata": {
        "id": "109baddd"
      },
      "source": [
        "### Feature Extraction and Array Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c994b52",
      "metadata": {
        "id": "7c994b52"
      },
      "outputs": [],
      "source": [
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "\n",
        "# 1. Get all unique class folder names and sort them alphabetically\n",
        "class_names = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
        "\n",
        "# 2. Create the dictionary\n",
        "label_map = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "FEATURE_OUTPUT_DIR = os.path.join('processed_data', 'class_splits')\n",
        "os.makedirs(FEATURE_OUTPUT_DIR, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "def create_and_save_features():\n",
        "    # List to hold file paths of NPY files for later concatenation\n",
        "    all_class_files = []\n",
        "\n",
        "    # Iterate through all class folders\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(DATA_ROOT, class_name)\n",
        "        label_index = label_map[class_name]\n",
        "\n",
        "        print(f\"Processing Class: {class_name} (Label: {label_index})\")\n",
        "\n",
        "        # --- Memory-Saving Block ---\n",
        "        class_keypoints = []\n",
        "        class_images = []\n",
        "        class_labels = []\n",
        "\n",
        "        for image_name in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "\n",
        "            # Use the imported modular function\n",
        "            keypoints, resized_img = extract_keypoints(image_path)\n",
        "\n",
        "            if keypoints is not None:\n",
        "                class_keypoints.append(keypoints)\n",
        "                class_images.append(resized_img)\n",
        "                class_labels.append(label_index)\n",
        "\n",
        "        # 3. Convert and Save (The memory-intensive part, done one class at a time)\n",
        "        X_key_class = np.array(class_keypoints, dtype=np.float32)\n",
        "        X_cnn_class = np.array(class_images, dtype=np.float32)\n",
        "        y_class = np.array(class_labels, dtype=np.int32)\n",
        "\n",
        "        # 4. Save to Disk\n",
        "        # Use a temporary name for each class file\n",
        "        key_file = os.path.join(FEATURE_OUTPUT_DIR, f'keypoints_{class_name}.npy')\n",
        "        cnn_file = os.path.join(FEATURE_OUTPUT_DIR, f'cnn_{class_name}.npy')\n",
        "        label_file = os.path.join(FEATURE_OUTPUT_DIR, f'labels_{class_name}.npy')\n",
        "\n",
        "        np.save(key_file, X_key_class)\n",
        "        np.save(cnn_file, X_cnn_class)\n",
        "        np.save(label_file, y_class)\n",
        "        all_class_files.append((key_file, cnn_file, label_file))\n",
        "\n",
        "        print(f\"Processed and saved {class_name}. Freeing memory...\")\n",
        "\n",
        "        # 5. Crucial: Delete objects and force garbage collection\n",
        "        del X_key_class, X_cnn_class, y_class, class_keypoints, class_images, class_labels\n",
        "        gc.collect()\n",
        "\n",
        "# --- EXECUTION ---\n",
        "create_and_save_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Concatenation"
      ],
      "metadata": {
        "id": "oIUXwOHPcOCY"
      },
      "id": "oIUXwOHPcOCY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting memory-optimized final concatenation...\")\n",
        "\n",
        "# 1. Identify all temporary class files that need to be merged\n",
        "temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "keypoint_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('keypoints_')]\n",
        "cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "\n",
        "# Check if files were found\n",
        "if not keypoint_files:\n",
        "    raise FileNotFoundError(\"No temporary keypoint files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not cnn_files:\n",
        "    raise FileNotFoundError(\"No temporary cnn files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not label_files:\n",
        "    raise FileNotFoundError(\"No temporary label files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "\n",
        "# 2. Memory-Optimized Concatenation (Loading one-by-one and overwriting)\n",
        "\n",
        "def merge_files_efficiently(file_list, final_name):\n",
        "    \"\"\"Loads files sequentially and saves the final result.\"\"\"\n",
        "\n",
        "    output_path = os.path.join(FEATURE_OUTPUT_DIR, final_name)\n",
        "    print(f\"Merging {len(file_list)} files into {final_name}...\")\n",
        "\n",
        "    all_arrays = [np.load(f) for f in file_list]\n",
        "    merged_array = np.concatenate(all_arrays)\n",
        "    np.save(output_path, merged_array)\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each merge\n",
        "    del all_arrays, merged_array\n",
        "    gc.collect()\n",
        "    print(f\"Successfully saved {final_name}.\")\n",
        "    return output_path\n",
        "\n",
        "# Execute the merges\n",
        "final_keypoints_path = merge_files_efficiently(keypoint_files, 'X_keypoints.npy')\n",
        "final_labels_path = merge_files_efficiently(label_files, 'y_labels.npy')\n",
        "\n",
        "print(\"\\nAll final feature files created successfully on local disk.\")\n",
        "\n",
        "# 3. Upload to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "print(f\"Uploading final processed features from {FEATURE_OUTPUT_DIR} to {GCS_PATH}...\")\n",
        "\n",
        "print(f\"Uploading final feature files to {GCS_PATH}...\")\n",
        "\n",
        "# Upload X_keypoints.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/X_keypoints.npy {GCS_PATH}/X_keypoints.npy\n",
        "\n",
        "# Upload y_labels.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/y_labels.npy {GCS_PATH}/y_labels.npy\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Only final files were uploaded.\")\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Data processing pipeline finished! ðŸŽ‰\")"
      ],
      "metadata": {
        "id": "1rfczEYtcNMr"
      },
      "id": "1rfczEYtcNMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Memory-Mapped Merge for X_cnn_images (with Disk Cleanup) ---\")\n",
        "\n",
        "# 1. Identify all temporary files and verify paths\n",
        "try:\n",
        "    temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "    # We load these lists for reference, they are NOT deleted yet.\n",
        "    label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "    cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The directory {FEATURE_OUTPUT_DIR} was not found. Please check REPO_NAME.\")\n",
        "    exit()\n",
        "\n",
        "if not cnn_files or not label_files:\n",
        "    print(\"Error: No intermediate 'cnn_*.npy' or 'labels_*.npy' files found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# 2. Calculate the required final shape (metadata only)\n",
        "print(f\"Found {len(cnn_files)} intermediate files.\")\n",
        "\n",
        "# Calculate the total number of samples (rows)\n",
        "total_samples = sum(np.load(f).shape[0] for f in label_files)\n",
        "\n",
        "# Get the shape of a single image (e.g., (224, 224, 3))\n",
        "cnn_image_shape = np.load(cnn_files[0]).shape[1:]\n",
        "\n",
        "print(f\"Total Samples to Merge: {total_samples}\")\n",
        "print(f\"Image Feature Shape: {cnn_image_shape}\")\n",
        "\n",
        "# 3. Create and Populate the Memory-Mapped Array\n",
        "FINAL_CNN_PATH = os.path.join(FEATURE_OUTPUT_DIR, 'X_cnn_images.npy')\n",
        "current_row = 0\n",
        "\n",
        "print(f\"Creating memory-mapped file at: {FINAL_CNN_PATH}\")\n",
        "\n",
        "# Create the destination memory-mapped array (mode='w+' means create/write)\n",
        "X_cnn_final_map = np.memmap(\n",
        "    FINAL_CNN_PATH,\n",
        "    dtype=np.float32,\n",
        "    mode='w+',\n",
        "    shape=(total_samples, *cnn_image_shape)\n",
        ")\n",
        "\n",
        "# Iteratively write data into the memory-mapped file\n",
        "for i, cnn_file in enumerate(cnn_files):\n",
        "    # Load one small class array into RAM\n",
        "    X_cnn_class = np.load(cnn_file)\n",
        "    num_samples = X_cnn_class.shape[0]\n",
        "\n",
        "    # Write the small array directly into the correct slice of the large file on disk\n",
        "    X_cnn_final_map[current_row:current_row + num_samples] = X_cnn_class\n",
        "\n",
        "    # Update the row counter\n",
        "    current_row += num_samples\n",
        "\n",
        "    print(f\"  -> Wrote file {i+1}/{len(cnn_files)} ({num_samples} samples).\")\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each loop\n",
        "    del X_cnn_class\n",
        "    gc.collect()\n",
        "\n",
        "    # Flush ensures data is written to disk immediately\n",
        "    X_cnn_final_map.flush()\n",
        "\n",
        "    # --- DISK CLEANUP STEP ---\n",
        "    os.remove(cnn_file)\n",
        "    print(f\"  -> Deleted source file: {os.path.basename(cnn_file)}\")\n",
        "\n",
        "print(\"\\nStep 1 of 2: X_cnn_images successfully merged and saved locally.\")\n",
        "\n",
        "# Final cleanup of the memmap object before GCS upload\n",
        "del X_cnn_final_map\n",
        "gc.collect()\n",
        "\n",
        "# 4. Upload the final file to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "GCS_DESTINATION_FILE = os.path.basename(FINAL_CNN_PATH)\n",
        "\n",
        "print(f\"\\nStep 2 of 2: Uploading {GCS_DESTINATION_FILE} to {GCS_PATH}...\")\n",
        "# Use gsutil cp to copy the local file to the GCS path\n",
        "!gsutil cp {FINAL_CNN_PATH} {GCS_PATH}/{GCS_DESTINATION_FILE}\n",
        "\n",
        "print(\"\\nSUCCESS: X_cnn_images.npy uploaded to GCS.\")"
      ],
      "metadata": {
        "id": "R2Q8P6u-jO_2"
      },
      "id": "R2Q8P6u-jO_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "%cd /\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "%cd /content/sample_data\n",
        "print(\"New working directory:\", os.getcwd())\n",
        "\n",
        "%cd /\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())"
      ],
      "metadata": {
        "id": "wRz_FMZbT2ql",
        "outputId": "c247eeef-0af5-49bc-f00e-8b3d3e0d4a9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wRz_FMZbT2ql",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Current working directory: /\n",
            "/content/sample_data\n",
            "New working directory: /content/sample_data\n",
            "/\n",
            "Current working directory: /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data: Loading and Splitting"
      ],
      "metadata": {
        "id": "x7bKz2iu9tH8"
      },
      "id": "x7bKz2iu9tH8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "hE6qsRjJcdbr"
      },
      "id": "hE6qsRjJcdbr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "%cd /content\n",
        "\n",
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1/\"\n",
        "CNN_FILE_NAME = \"X_cnn_images.npy\"\n",
        "LABELS_FILE_NAME = \"y_labels.npy\"\n",
        "LOCAL_FEATURE_DIR = 'gcs_loaded_data'\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n"
      ],
      "metadata": {
        "id": "HXEDMgUXWVlz",
        "outputId": "9ad7373f-0313-4760-cb5c-26b8ad08766d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HXEDMgUXWVlz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_gcs_data():\n",
        "    \"\"\"Authenticates GCS access and copies large files to the local Colab SSD.\"\"\"\n",
        "    print(\"Authenticating Google Cloud Storage...\")\n",
        "    try:\n",
        "        # Authenticate the user for GCS access\n",
        "        auth.authenticate_user()\n",
        "    except Exception as e:\n",
        "        print(f\"Authentication failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Create the local directory\n",
        "    os.makedirs(LOCAL_FEATURE_DIR, exist_ok=True)\n",
        "    print(f\"Local storage directory created at: {LOCAL_FEATURE_DIR}\")\n",
        "\n",
        "    # Use gsutil to copy the files to the local SSD\n",
        "    print(f\"Copying {CNN_FILE_NAME} (38 GB) from GCS to local SSD...\")\n",
        "    # It is crucial to use the local SSD for fast I/O during training.\n",
        "    # The 'gsutil cp' command is optimized for this transfer.\n",
        "    try:\n",
        "        # Copy the large feature file\n",
        "        !gsutil cp {GCS_PATH}{CNN_FILE_NAME} {LOCAL_FEATURE_DIR}/\n",
        "\n",
        "        # Copy the much smaller labels file\n",
        "        print(f\"Copying {LABELS_FILE_NAME} from GCS to local SSD...\")\n",
        "        !gsutil cp {GCS_PATH}{LABELS_FILE_NAME} {LOCAL_FEATURE_DIR}/\n",
        "        print(\"Data transfer complete.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Data transfer failed: {e}\")\n",
        "        return False\n",
        "\n",
        "setup_gcs_data()"
      ],
      "metadata": {
        "id": "A5WrkuYxQY4E",
        "outputId": "4bd7eb33-4334-4084-c8e9-ba809d6ebb1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A5WrkuYxQY4E",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticating Google Cloud Storage...\n",
            "Local storage directory created at: gcs_loaded_data\n",
            "Copying X_cnn_images.npy (38 GB) from GCS to local SSD...\n",
            "Copying gs://asl-keypoint-data-storage-2025/processed_features_v1/X_cnn_images.npy...\n",
            "==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "\\ [1 files][ 35.7 GiB/ 35.7 GiB]  122.8 MiB/s                                   \n",
            "Operation completed over 1 objects/35.7 GiB.                                     \n",
            "Copying y_labels.npy from GCS to local SSD...\n",
            "Copying gs://asl-keypoint-data-storage-2025/processed_features_v1/y_labels.npy...\n",
            "/ [1 files][248.9 KiB/248.9 KiB]                                                \n",
            "Operation completed over 1 objects/248.9 KiB.                                    \n",
            "Data transfer complete.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrkyHvsQ9wyM"
      },
      "id": "WrkyHvsQ9wyM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}