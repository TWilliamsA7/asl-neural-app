{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d3d31e4",
      "metadata": {
        "id": "3d3d31e4"
      },
      "source": [
        "# ASL Neural Network Pipeline Notebook\n",
        "\n",
        "This notebook contains all the steps necessary to train a neural network for the ASL Neural Network App project located at [this repository](https://github.com/TWilliamsA7/asl-neural-app/tree/main). Utility functions can also be found in the above repository under the src directory.\n",
        "\n",
        "1. Setup: Configuration & Authentication\n",
        "2. Environment: Initialization & Imports\n",
        "3. Data: Acquisition & Preprocessing\n",
        "4. Data: Loading & Splitting\n",
        "5. Model: Architecture\n",
        "6. Model: Training\n",
        "7. Model: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc88ffa",
      "metadata": {
        "id": "8fc88ffa"
      },
      "source": [
        "## Setup: Configuration & Authenticatioon\n",
        "\n",
        "This section of the notebook is for setting up the necessary authentication and configuration of the Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a5c745b2",
      "metadata": {
        "id": "a5c745b2"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules for setup\n",
        "\n",
        "from google.colab import userdata, auth, files\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523bde30",
      "metadata": {
        "id": "523bde30"
      },
      "source": [
        "### Create github connection via colab variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e7289b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7289b94",
        "outputId": "9964c80d-522a-41e2-8e1e-dcfd3da97de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup github connection and authenticated url successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define repository details\n",
        "USERNAME = \"TWilliamsA7\"\n",
        "REPO_NAME = \"asl-neural-app.git\"\n",
        "BRANCH_NAME = \"main\"\n",
        "\n",
        "# Get PAT (Personal Access Token) stored in Colab Secrets\n",
        "PAT = userdata.get(\"GITHUB_PAT\")\n",
        "if not PAT:\n",
        "    raise ValueError(\"GITHUB_PAT secret not found!\")\n",
        "\n",
        "# Construct Authetnicated URL for accessing repositry\n",
        "AUTHENTICATED_URL = f\"https://{PAT}@github.com/{USERNAME}/{REPO_NAME}\"\n",
        "REPO_FOLDER = REPO_NAME.replace(\".git\", \"\")\n",
        "\n",
        "# Set golabl Git configuration\n",
        "!git config --global user.email \"twilliamsa776@gmail.com\"\n",
        "!git config --global user.name \"{USERNAME}\"\n",
        "\n",
        "print(\"Setup github connection and authenticated url successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242d89e6",
      "metadata": {
        "id": "242d89e6"
      },
      "source": [
        "### Google Cloud Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54ae645b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ae645b",
        "outputId": "18c29a2a-488d-4e6f-d78b-a23290f293db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GCS Authentication ---\n",
            "Google Cloud authentication complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"--- GCS Authentication ---\")\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Google Cloud authentication complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10250879",
      "metadata": {
        "id": "10250879"
      },
      "source": [
        "## Environment: Initialization and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb5f491",
      "metadata": {
        "id": "ccb5f491"
      },
      "source": [
        "### Clone Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c52c298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c52c298",
        "outputId": "0eb0d3f0-1455-41e0-ddb9-26aea85a190a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing old asl-neural-app folder...\n",
            "Cloning repository: asl-neural-app.git...\n",
            "Cloning into 'asl-neural-app'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 97 (delta 42), reused 78 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 23.41 KiB | 4.68 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n",
            "/content/asl-neural-app\n",
            "Current working directory: /content/asl-neural-app\n"
          ]
        }
      ],
      "source": [
        "# Clean up any existing clone (optional, but good for reliable restarts)\n",
        "if os.path.isdir(REPO_FOLDER):\n",
        "    print(f\"Removing old {REPO_FOLDER} folder...\")\n",
        "    !rm -rf {REPO_FOLDER}\n",
        "\n",
        "# Clone the repository using the authenticated URL\n",
        "print(f\"Cloning repository: {REPO_NAME}...\")\n",
        "!git clone {AUTHENTICATED_URL}\n",
        "\n",
        "# Change directory into the cloned repository\n",
        "%cd {REPO_FOLDER}\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1e3b2f",
      "metadata": {
        "id": "6d1e3b2f"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        "- Includes manual inclusion of kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "41e364e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e364e0",
        "outputId": "1bd64cce-b349-46cf-9159-934330a84ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upgrading pip, setuptools, and wheel...\n",
            "Using preinstalled numpy and tensorflow dependencies\n",
            "Installing remaining project dependencies from requirements.txt...\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Upgrading pip, setuptools, and wheel...\")\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "print(\"Using preinstalled numpy and tensorflow dependencies\")\n",
        "\n",
        "print(\"Installing remaining project dependencies from requirements.txt...\")\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup .Kaggle Directory"
      ],
      "metadata": {
        "id": "UxPQq8J-11mw"
      },
      "id": "UxPQq8J-11mw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the credentials file already exists in the expected location\n",
        "if not os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
        "    print(\"Uploading kaggle.json file...\")\n",
        "    # This will open a dialog for you to select and upload your file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Check if the upload was successful\n",
        "    if not uploaded:\n",
        "        print(\"ERROR: kaggle.json was not uploaded.\")\n",
        "    else:\n",
        "        # The uploaded file is now in the current working directory (/content/)\n",
        "        # Proceed to move and secure it.\n",
        "\n",
        "        # 2. Create the required directory\n",
        "        !mkdir -p ~/.kaggle/\n",
        "\n",
        "        # 3. Move the uploaded file into the correct directory\n",
        "        # The key in the uploaded dictionary is the filename (kaggle.json)\n",
        "        # We assume the user uploaded the file named 'kaggle.json'\n",
        "        !mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "        # 4. Set the correct permissions (CRITICAL)\n",
        "        # Permissions MUST be 600 for security.\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "        print(\"Kaggle authentication file set up successfully!\")\n",
        "else:\n",
        "    print(\"Kaggle credentials already found at ~/.kaggle/kaggle.json.\")\n",
        "\n",
        "# --- Verification Step ---\n",
        "# Run a simple Kaggle command to test authentication\n",
        "try:\n",
        "    print(\"\\nAttempting to list datasets (Verification)...\")\n",
        "    # This command uses the username/key from the now-configured kaggle.json\n",
        "    !kaggle datasets list -s asl_alphabet | head -n 3\n",
        "    print(\"\\nSUCCESS: Kaggle API authenticated and is functional.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Verification failed. Please check the content of your kaggle.json file. Details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJxd0qhK11Mi",
        "outputId": "a93a6e69-9e4a-4cfc-92fc-fd0bd470bbd0"
      },
      "id": "KJxd0qhK11Mi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials already found at ~/.kaggle/kaggle.json.\n",
            "\n",
            "Attempting to list datasets (Verification)...\n",
            "ref                                      title                                 size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------  ------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "baoanhcr7/asl-alphabet                   ASL_alphabet                    1100898315  2021-11-06 20:56:29.557000            231          1  0.125            \n",
            "\n",
            "SUCCESS: Kaggle API authenticated and is functional.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f773ce51",
      "metadata": {
        "id": "f773ce51"
      },
      "source": [
        "### Connect Src directory for access to utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "15926923",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15926923",
        "outputId": "7d538e30-634d-4716-8948-0ecbc91ff4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup Complete. Colab environment is ready.\n"
          ]
        }
      ],
      "source": [
        "sys.path.append('src')\n",
        "print(\"Setup Complete. Colab environment is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c389e19f",
      "metadata": {
        "id": "c389e19f"
      },
      "source": [
        "## Data: Acquisition & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0339e4f8",
      "metadata": {
        "id": "0339e4f8"
      },
      "source": [
        "### Include necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d219f52",
      "metadata": {
        "id": "1d219f52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "# If earlier cells are not ran\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure src accessibility\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import utility functions\n",
        "from data_utils import extract_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112699ad",
      "metadata": {
        "id": "112699ad"
      },
      "source": [
        "### Setup directories and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d527d2c4",
      "metadata": {
        "id": "d527d2c4"
      },
      "outputs": [],
      "source": [
        "KAGGLE_DATASET_ID = \"grassknoted/asl-alphabet\"\n",
        "DESTINATION_PATH = \"sample_data\"\n",
        "PROCESSED_OUTPUT_DIR = 'processed_data'\n",
        "DATA_ROOT_FOLDER_NAME = 'asl_alphabet_train'\n",
        "\n",
        "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
        "os.makedirs(PROCESSED_OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1419d35",
      "metadata": {
        "id": "a1419d35"
      },
      "source": [
        "### Download Data via Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "719c8f59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719c8f59",
        "outputId": "04957947-b9c2-4d5d-dab7-9b41a482a88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset: grassknoted/asl-alphabet\n",
            "Dataset URL: https://www.kaggle.com/datasets/grassknoted/asl-alphabet\n",
            "License(s): GPL-2.0\n",
            "Downloading asl-alphabet.zip to sample_data\n",
            " 99% 1.01G/1.03G [00:21<00:00, 77.8MB/s]\n",
            "100% 1.03G/1.03G [00:21<00:00, 51.9MB/s]\n",
            "Image data root set to: sample_data/asl_alphabet_train/asl_alphabet_train\n"
          ]
        }
      ],
      "source": [
        "print(f\"Downloading dataset: {KAGGLE_DATASET_ID}\")\n",
        "!kaggle datasets download -d {KAGGLE_DATASET_ID} -p {DESTINATION_PATH} --unzip\n",
        "\n",
        "# Define the exact root path to the image subfolders (A, B, C, etc.)\n",
        "DATA_ROOT = os.path.join(DESTINATION_PATH, DATA_ROOT_FOLDER_NAME, DATA_ROOT_FOLDER_NAME)\n",
        "print(f\"Image data root set to: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109baddd",
      "metadata": {
        "id": "109baddd"
      },
      "source": [
        "### Feature Extraction and Array Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c994b52",
      "metadata": {
        "id": "7c994b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db551e54-3103-45c5-dd7d-a3587e63dfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Class: A (Label: 0)\n",
            "Initializing MediaPipe Hands Detector...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved A. Freeing memory...\n",
            "Processing Class: B (Label: 1)\n",
            "Processed and saved B. Freeing memory...\n",
            "Processing Class: C (Label: 2)\n",
            "Processed and saved C. Freeing memory...\n",
            "Processing Class: D (Label: 3)\n",
            "Processed and saved D. Freeing memory...\n",
            "Processing Class: E (Label: 4)\n",
            "Processed and saved E. Freeing memory...\n",
            "Processing Class: F (Label: 5)\n",
            "Processed and saved F. Freeing memory...\n",
            "Processing Class: G (Label: 6)\n",
            "Processed and saved G. Freeing memory...\n",
            "Processing Class: H (Label: 7)\n",
            "Processed and saved H. Freeing memory...\n",
            "Processing Class: I (Label: 8)\n",
            "Processed and saved I. Freeing memory...\n",
            "Processing Class: J (Label: 9)\n",
            "Processed and saved J. Freeing memory...\n",
            "Processing Class: K (Label: 10)\n",
            "Processed and saved K. Freeing memory...\n",
            "Processing Class: L (Label: 11)\n",
            "Processed and saved L. Freeing memory...\n",
            "Processing Class: M (Label: 12)\n",
            "Processed and saved M. Freeing memory...\n",
            "Processing Class: N (Label: 13)\n",
            "Processed and saved N. Freeing memory...\n",
            "Processing Class: O (Label: 14)\n",
            "Processed and saved O. Freeing memory...\n",
            "Processing Class: P (Label: 15)\n",
            "Processed and saved P. Freeing memory...\n",
            "Processing Class: Q (Label: 16)\n",
            "Processed and saved Q. Freeing memory...\n",
            "Processing Class: R (Label: 17)\n",
            "Processed and saved R. Freeing memory...\n",
            "Processing Class: S (Label: 18)\n",
            "Processed and saved S. Freeing memory...\n",
            "Processing Class: T (Label: 19)\n",
            "Processed and saved T. Freeing memory...\n",
            "Processing Class: U (Label: 20)\n",
            "Processed and saved U. Freeing memory...\n",
            "Processing Class: V (Label: 21)\n",
            "Processed and saved V. Freeing memory...\n",
            "Processing Class: W (Label: 22)\n",
            "Processed and saved W. Freeing memory...\n",
            "Processing Class: X (Label: 23)\n",
            "Processed and saved X. Freeing memory...\n",
            "Processing Class: Y (Label: 24)\n",
            "Processed and saved Y. Freeing memory...\n",
            "Processing Class: Z (Label: 25)\n",
            "Processed and saved Z. Freeing memory...\n",
            "Processing Class: del (Label: 26)\n",
            "Processed and saved del. Freeing memory...\n",
            "Processing Class: nothing (Label: 27)\n",
            "Processed and saved nothing. Freeing memory...\n",
            "Processing Class: space (Label: 28)\n",
            "Processed and saved space. Freeing memory...\n"
          ]
        }
      ],
      "source": [
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "\n",
        "# 1. Get all unique class folder names and sort them alphabetically\n",
        "class_names = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
        "\n",
        "# 2. Create the dictionary\n",
        "label_map = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "FEATURE_OUTPUT_DIR = os.path.join('processed_data', 'class_splits')\n",
        "os.makedirs(FEATURE_OUTPUT_DIR, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "def create_and_save_features():\n",
        "    # List to hold file paths of NPY files for later concatenation\n",
        "    all_class_files = []\n",
        "\n",
        "    # Iterate through all class folders\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(DATA_ROOT, class_name)\n",
        "        label_index = label_map[class_name]\n",
        "\n",
        "        print(f\"Processing Class: {class_name} (Label: {label_index})\")\n",
        "\n",
        "        # --- Memory-Saving Block ---\n",
        "        class_keypoints = []\n",
        "        class_images = []\n",
        "        class_labels = []\n",
        "\n",
        "        for image_name in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "\n",
        "            # Use the imported modular function\n",
        "            keypoints, resized_img = extract_keypoints(image_path)\n",
        "\n",
        "            if keypoints is not None:\n",
        "                class_keypoints.append(keypoints)\n",
        "                class_images.append(resized_img)\n",
        "                class_labels.append(label_index)\n",
        "\n",
        "        # 3. Convert and Save (The memory-intensive part, done one class at a time)\n",
        "        X_key_class = np.array(class_keypoints, dtype=np.float32)\n",
        "        X_cnn_class = np.array(class_images, dtype=np.float32)\n",
        "        y_class = np.array(class_labels, dtype=np.int32)\n",
        "\n",
        "        # 4. Save to Disk\n",
        "        # Use a temporary name for each class file\n",
        "        key_file = os.path.join(FEATURE_OUTPUT_DIR, f'keypoints_{class_name}.npy')\n",
        "        cnn_file = os.path.join(FEATURE_OUTPUT_DIR, f'cnn_{class_name}.npy')\n",
        "        label_file = os.path.join(FEATURE_OUTPUT_DIR, f'labels_{class_name}.npy')\n",
        "\n",
        "        np.save(key_file, X_key_class)\n",
        "        np.save(cnn_file, X_cnn_class)\n",
        "        np.save(label_file, y_class)\n",
        "        all_class_files.append((key_file, cnn_file, label_file))\n",
        "\n",
        "        print(f\"Processed and saved {class_name}. Freeing memory...\")\n",
        "\n",
        "        # 5. Crucial: Delete objects and force garbage collection\n",
        "        del X_key_class, X_cnn_class, y_class, class_keypoints, class_images, class_labels\n",
        "        gc.collect()\n",
        "\n",
        "    # --- Final Step: Concatenate all saved files ---\n",
        "    # This part is still memory-intensive, but happens once at the end\n",
        "    final_keypoints = np.concatenate([np.load(f[0]) for f in all_class_files])\n",
        "    final_cnn_images = np.concatenate([np.load(f[1]) for f in all_class_files])\n",
        "    final_labels = np.concatenate([np.load(f[2]) for f in all_class_files])\n",
        "\n",
        "    # Save final concatenated arrays (as you did before)\n",
        "    np.save(os.path.join(FEATURE_OUTPUT_DIR, 'X_keypoints.npy'), final_keypoints)\n",
        "    np.save(os.path.join(FEATURE_OUTPUT_DIR, 'X_cnn_images.npy'), final_cnn_images)\n",
        "    np.save(os.path.join(FEATURE_OUTPUT_DIR, 'y_labels.npy'), final_labels)\n",
        "\n",
        "    # Source is the local temp directory. Destination is the GCS path.\n",
        "    GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "    print(f\"\\nUploading processed features to {GCS_PATH}...\")\n",
        "\n",
        "    # The -m flag runs the command multi-threaded (faster) and -r copies the directory recursively\n",
        "    !gsutil -m cp -r {FEATURE_OUTPUT_DIR} {GCS_PATH}\n",
        "\n",
        "    print(\"\\nUpload to GCS complete. Features are ready for training notebook.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "create_and_save_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Concatenation"
      ],
      "metadata": {
        "id": "oIUXwOHPcOCY"
      },
      "id": "oIUXwOHPcOCY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Python CWD:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Y7rSBggWMm",
        "outputId": "f46c12b7-8c87-46be-c8ed-252b033c20ea"
      },
      "id": "V6Y7rSBggWMm",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python CWD: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "REPO_NAME = 'asl-neural-app'\n",
        "FEATURE_OUTPUT_DIR = os.path.join('/content/', REPO_NAME, 'processed_data', 'class_splits')\n",
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "\n",
        "print(\"Starting memory-optimized final concatenation...\")\n",
        "\n",
        "# 1. Identify all temporary class files that need to be merged\n",
        "temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "keypoint_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('keypoints_')]\n",
        "cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "\n",
        "# Check if files were found\n",
        "if not keypoint_files:\n",
        "    raise FileNotFoundError(\"No temporary keypoint files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not cnn_files:\n",
        "    raise FileNotFoundError(\"No temporary cnn files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "if not label_files:\n",
        "    raise FileNotFoundError(\"No temporary label files found. Check FEATURE_OUTPUT_DIR path.\")\n",
        "\n",
        "# 2. Memory-Optimized Concatenation (Loading one-by-one and overwriting)\n",
        "\n",
        "def merge_files_efficiently(file_list, final_name):\n",
        "    \"\"\"Loads files sequentially and saves the final result.\"\"\"\n",
        "\n",
        "    output_path = os.path.join(FEATURE_OUTPUT_DIR, final_name)\n",
        "    print(f\"Merging {len(file_list)} files into {final_name}...\")\n",
        "\n",
        "    all_arrays = [np.load(f) for f in file_list]\n",
        "    merged_array = np.concatenate(all_arrays)\n",
        "    np.save(output_path, merged_array)\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each merge\n",
        "    del all_arrays, merged_array\n",
        "    gc.collect()\n",
        "    print(f\"Successfully saved {final_name}.\")\n",
        "    return output_path\n",
        "\n",
        "# Execute the merges\n",
        "final_keypoints_path = merge_files_efficiently(keypoint_files, 'X_keypoints.npy')\n",
        "final_cnn_path = merge_files_efficiently(cnn_files, 'X_cnn_images.npy')\n",
        "final_labels_path = merge_files_efficiently(label_files, 'y_labels.npy')\n",
        "\n",
        "print(\"\\nAll final feature files created successfully on local disk.\")\n",
        "\n",
        "# 3. Upload to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "print(f\"Uploading final processed features from {FEATURE_OUTPUT_DIR} to {GCS_PATH}...\")\n",
        "\n",
        "print(f\"Uploading final feature files to {GCS_PATH}...\")\n",
        "\n",
        "# 1. Upload X_keypoints.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/X_keypoints.npy {GCS_PATH}/X_keypoints.npy\n",
        "\n",
        "# 2. Upload X_cnn_images.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/X_cnn_images.npy {GCS_PATH}/X_cnn_images.npy\n",
        "\n",
        "# 3. Upload y_labels.npy\n",
        "!gsutil cp {FEATURE_OUTPUT_DIR}/y_labels.npy {GCS_PATH}/y_labels.npy\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Only final files were uploaded.\")\n",
        "\n",
        "print(\"\\nUpload to GCS complete. Data processing pipeline finished! ðŸŽ‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rfczEYtcNMr",
        "outputId": "7b4f35a4-64f2-492f-b9f8-592c99b09a0f"
      },
      "id": "1rfczEYtcNMr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting memory-optimized final concatenation...\n",
            "Merging 29 files into X_keypoints.npy...\n",
            "Successfully saved X_keypoints.npy.\n",
            "Merging 29 files into X_cnn_images.npy...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# --- CONFIGURATION (PLEASE VERIFY THESE PATHS) ---\n",
        "# Replace 'your_repo_name' with your actual cloned repository name if using absolute path\n",
        "REPO_NAME = 'asl-neural-app'\n",
        "FEATURE_OUTPUT_DIR = os.path.join('/content/', REPO_NAME, 'processed_data', 'class_splits')\n",
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "# --------------------------------------------------\n",
        "\n",
        "print(\"--- Starting Memory-Mapped Merge for X_cnn_images ---\")\n",
        "\n",
        "# 1. Identify all temporary files and verify paths\n",
        "try:\n",
        "    temp_files = sorted(os.listdir(FEATURE_OUTPUT_DIR))\n",
        "    label_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('labels_')]\n",
        "    cnn_files = [os.path.join(FEATURE_OUTPUT_DIR, f) for f in temp_files if f.startswith('cnn_')]\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The directory {FEATURE_OUTPUT_DIR} was not found. Please check REPO_NAME.\")\n",
        "    exit()\n",
        "\n",
        "if not cnn_files or not label_files:\n",
        "    print(\"Error: No intermediate 'cnn_*.npy' or 'labels_*.npy' files found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# 2. Calculate the required final shape (must be done in RAM, but only metadata)\n",
        "print(f\"Found {len(cnn_files)} intermediate files.\")\n",
        "\n",
        "# Calculate the total number of samples (rows)\n",
        "total_samples = sum(np.load(f).shape[0] for f in label_files)\n",
        "\n",
        "# Get the shape of a single image (e.g., (224, 224, 3))\n",
        "cnn_image_shape = np.load(cnn_files[0]).shape[1:]\n",
        "\n",
        "print(f\"Total Samples to Merge: {total_samples}\")\n",
        "print(f\"Image Feature Shape: {cnn_image_shape}\")\n",
        "\n",
        "# 3. Create and Populate the Memory-Mapped Array\n",
        "FINAL_CNN_PATH = os.path.join(FEATURE_OUTPUT_DIR, 'X_cnn_images.npy')\n",
        "current_row = 0\n",
        "\n",
        "print(f\"Creating memory-mapped file at: {FINAL_CNN_PATH}\")\n",
        "\n",
        "# Create the destination memory-mapped array (mode='w+' means create/write)\n",
        "X_cnn_final_map = np.memmap(\n",
        "    FINAL_CNN_PATH,\n",
        "    dtype=np.float32,\n",
        "    mode='w+',\n",
        "    shape=(total_samples, *cnn_image_shape)\n",
        ")\n",
        "\n",
        "# Iteratively write data into the memory-mapped file\n",
        "for i, cnn_file in enumerate(cnn_files):\n",
        "    # Load one small class array into RAM\n",
        "    X_cnn_class = np.load(cnn_file)\n",
        "    num_samples = X_cnn_class.shape[0]\n",
        "\n",
        "    # Write the small array directly into the correct slice of the large file on disk\n",
        "    X_cnn_final_map[current_row:current_row + num_samples] = X_cnn_class\n",
        "\n",
        "    # Update the row counter\n",
        "    current_row += num_samples\n",
        "\n",
        "    print(f\"  -> Wrote file {i+1}/{len(cnn_files)} ({num_samples} samples)\")\n",
        "\n",
        "    # Crucial: Delete objects and force garbage collection after each loop\n",
        "    del X_cnn_class\n",
        "    gc.collect()\n",
        "\n",
        "    # Flush ensures data is written to disk immediately\n",
        "    X_cnn_final_map.flush()\n",
        "\n",
        "print(\"\\nStep 1 of 2: X_cnn_images successfully merged and saved locally.\")\n",
        "\n",
        "# Final cleanup of the memmap object before GCS upload\n",
        "del X_cnn_final_map\n",
        "gc.collect()\n",
        "\n",
        "# 4. Upload the final file to GCS\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
        "GCS_DESTINATION_FILE = os.path.basename(FINAL_CNN_PATH)\n",
        "\n",
        "print(f\"\\nStep 2 of 2: Uploading {GCS_DESTINATION_FILE} to {GCS_PATH}...\")\n",
        "# Use gsutil cp to copy the local file to the GCS path\n",
        "!gsutil cp {FINAL_CNN_PATH} {GCS_PATH}/{GCS_DESTINATION_FILE}\n",
        "\n",
        "print(\"\\nSUCCESS: X_cnn_images.npy uploaded to GCS. Ready to move to keypoints and labels.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "R2Q8P6u-jO_2",
        "outputId": "c139c476-d09b-4b8b-d4e1-5021206725a5"
      },
      "id": "R2Q8P6u-jO_2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Memory-Mapped Merge for X_cnn_images ---\n",
            "Found 29 intermediate files.\n",
            "Total Samples to Merge: 63676\n",
            "Image Feature Shape: (224, 224, 3)\n",
            "Creating memory-mapped file at: /content/asl-neural-app/processed_data/class_splits/X_cnn_images.npy\n",
            "  -> Wrote file 1/29 (2187 samples)\n",
            "  -> Wrote file 2/29 (2207 samples)\n",
            "  -> Wrote file 3/29 (1988 samples)\n",
            "  -> Wrote file 4/29 (2463 samples)\n",
            "  -> Wrote file 5/29 (2308 samples)\n",
            "  -> Wrote file 6/29 (2876 samples)\n",
            "  -> Wrote file 7/29 (2440 samples)\n",
            "  -> Wrote file 8/29 (2393 samples)\n",
            "  -> Wrote file 9/29 (2384 samples)\n",
            "  -> Wrote file 10/29 (2578 samples)\n",
            "  -> Wrote file 11/29 (2700 samples)\n",
            "  -> Wrote file 12/29 (2527 samples)\n",
            "  -> Wrote file 13/29 (1565 samples)\n",
            "  -> Wrote file 14/29 (1276 samples)\n",
            "  -> Wrote file 15/29 (2265 samples)\n",
            "  -> Wrote file 16/29 (2042 samples)\n",
            "  -> Wrote file 17/29 (2093 samples)\n",
            "  -> Wrote file 18/29 (2541 samples)\n",
            "  -> Wrote file 19/29 (2551 samples)\n",
            "  -> Wrote file 20/29 (2349 samples)\n",
            "  -> Wrote file 21/29 (2516 samples)\n",
            "  -> Wrote file 22/29 (2548 samples)\n",
            "  -> Wrote file 23/29 (2456 samples)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3911813895.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mcurrent_row\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  -> Wrote file {i+1}/{len(cnn_files)} ({num_samples} samples)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Crucial: Delete objects and force garbage collection after each loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data: Loading and Splitting"
      ],
      "metadata": {
        "id": "x7bKz2iu9tH8"
      },
      "id": "x7bKz2iu9tH8"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration (Must match Phase 2 upload settings) ---\n",
        "GCS_BUCKET_NAME = \"gs://asl-keypoint-data-storage-2025\"\n",
        "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
        "LOCAL_LOAD_DIR = 'gcs_loaded_data'\n",
        "TEMP_FEATURE_DIR = os.path.join(LOCAL_LOAD_DIR, GCS_DESTINATION_FOLDER, 'temp_feature_dump')\n",
        "\n",
        "os.makedirs(LOCAL_LOAD_DIR, exist_ok=True)\n",
        "\n",
        "GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}/temp_feature_dump\"\n",
        "print(f\"Downloading processed features from {GCS_PATH}...\")\n",
        "\n",
        "# Download the files recursively from the GCS folder to the local directory\n",
        "!gsutil -m cp -r {GCS_PATH} {LOCAL_LOAD_DIR}\n",
        "\n",
        "print(\"Download complete. Loading arrays into memory...\")\n",
        "\n",
        "# Load the data arrays (These files were saved in the temp_feature_dump folder)\n",
        "X_keypoints = np.load(os.path.join(TEMP_FEATURE_DIR, 'X_keypoints.npy'))\n",
        "X_cnn_images = np.load(os.path.join(TEMP_FEATURE_DIR, 'X_cnn_images.npy'))\n",
        "y_labels = np.load(os.path.join(TEMP_FEATURE_DIR, 'y_labels.npy'))\n",
        "\n",
        "print(f\"Total Samples Loaded: {X_keypoints.shape[0]}\")"
      ],
      "metadata": {
        "id": "WrkyHvsQ9wyM"
      },
      "id": "WrkyHvsQ9wyM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}