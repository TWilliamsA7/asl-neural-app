{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5568e1bc",
   "metadata": {},
   "source": [
    "# Data Preprocssing Notebook\n",
    "- Download Kaggle Dataset and use Mediapipe to preprocess images\n",
    "- Save features and landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93dde0",
   "metadata": {},
   "source": [
    "## Configurations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# If not already accessible\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import utility function\n",
    "from data_utils import extract_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (Adjust the Kaggle path as needed) ---\n",
    "KAGGLE_DATASET_ID = \"grassknoted/asl-alphabet\" \n",
    "DESTINATION_PATH = \"sample_data\"\n",
    "PROCESSED_OUTPUT_DIR = 'processed_data'\n",
    "DATA_ROOT_FOLDER_NAME = 'asl_alphabet_train' # Common folder name after unzipping\n",
    "\n",
    "os.makedirs(DESTINATION_PATH, exist_ok=True)\n",
    "os.makedirs(PROCESSED_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147c8a0",
   "metadata": {},
   "source": [
    "## Download Data via Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f88dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading dataset: {KAGGLE_DATASET_ID}\")\n",
    "!kaggle datasets download -d {KAGGLE_DATASET_ID} -p {DESTINATION_PATH} --unzip\n",
    "\n",
    "# Define the exact root path to the image subfolders (A, B, C, etc.)\n",
    "DATA_ROOT = os.path.join(DESTINATION_PATH, DATA_ROOT_FOLDER_NAME)\n",
    "print(f\"Image data root set to: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5b4eb",
   "metadata": {},
   "source": [
    "## Feature Extraction and Array Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET_NAME = \"gs://your-handsign-recognition-bucket\" \n",
    "GCS_DESTINATION_FOLDER = \"processed_features_v1\"\n",
    "\n",
    "def create_and_save_features():\n",
    "    X_keypoints, X_cnn, y_labels = [], [], []\n",
    "    \n",
    "    # Iterate through all class folders\n",
    "    for label_index, class_name in enumerate(sorted(os.listdir(DATA_ROOT))):\n",
    "        class_path = os.path.join(DATA_ROOT, class_name)\n",
    "        if not os.path.isdir(class_path) or class_name.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing Class: {class_name} (Label: {label_index})\")\n",
    "        \n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            \n",
    "            # Use the imported modular function\n",
    "            keypoints, resized_img = extract_keypoints(image_path)\n",
    "            \n",
    "            if keypoints is not None:\n",
    "                # Store keypoints\n",
    "                X_keypoints.append(keypoints)\n",
    "                # Normalize and store CNN image data\n",
    "                X_cnn.append(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB) / 255.0) \n",
    "                y_labels.append(label_index)\n",
    "\n",
    "    # Convert to final NumPy arrays\n",
    "    X_keypoints_array = np.array(X_keypoints, dtype=np.float32)\n",
    "    X_cnn_array = np.array(X_cnn, dtype=np.float32)\n",
    "    y_labels_array = np.array(y_labels, dtype=np.int32)\n",
    "\n",
    "    TEMP_DIR = 'temp_feature_dump'\n",
    "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "    # Save the processed data to the designated output folder\n",
    "    np.save(os.path.join(TEMP_DIR, 'X_keypoints.npy'), X_keypoints_array)\n",
    "    np.save(os.path.join(TEMP_DIR, 'X_cnn_images.npy'), X_cnn_array)\n",
    "    np.save(os.path.join(TEMP_DIR, 'y_labels.npy'), y_labels_array)\n",
    "    \n",
    "    # Source is the local temp directory. Destination is the GCS path.\n",
    "    GCS_PATH = f\"{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}\"\n",
    "    print(f\"\\nUploading processed features to {GCS_PATH}...\")\n",
    "    \n",
    "    # The -m flag runs the command multi-threaded (faster) and -r copies the directory recursively\n",
    "    !gsutil -m cp -r {TEMP_DIR} {GCS_PATH}\n",
    "    \n",
    "    print(\"\\nUpload to GCS complete. Features are ready for training notebook.\")\n",
    "\n",
    "    print(f\"\\nSuccessfully processed {len(X_keypoints)} samples.\")\n",
    "    print(f\"Keypoints Shape: {X_keypoints_array.shape}, Images Shape: {X_cnn_array.shape}\")\n",
    "    print(\"All processed arrays saved to the 'processed_data' directory.\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "create_and_save_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e2787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
